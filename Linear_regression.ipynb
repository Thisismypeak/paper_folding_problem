{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d078bb-a9b0-4ff9-970f-877eae1a7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb17a34-6b3d-49b3-80d4-c526dccb8e6c",
   "metadata": {},
   "source": [
    "#### Linear Regression Stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "458de990-a54f-4ac4-bdf6-f1ffcdf25637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69204b12-36ec-448f-b82a-418e5307ad2b",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80ad461c-41f0-4a80-b9ee-729693066646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"data/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "831cd9f0-c467-4676-90e6-91c1bd7a1873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 130.66594179083992\n",
      "0-th epoch val loss 130.5291659908649\n",
      "1-th epoch train loss 126.59821526520714\n",
      "1-th epoch val loss 126.46427964849454\n",
      "2-th epoch train loss 122.65752925605253\n",
      "2-th epoch val loss 122.52636189103059\n",
      "3-th epoch train loss 118.83992460970106\n",
      "3-th epoch val loss 118.71145554481132\n",
      "4-th epoch train loss 115.14156542501537\n",
      "4-th epoch val loss 115.01572663116491\n",
      "5-th epoch train loss 111.55873521848861\n",
      "5-th epoch val loss 111.4354605332292\n",
      "6-th epoch train loss 108.08783320862567\n",
      "6-th epoch val loss 107.96705828200632\n",
      "7-th epoch train loss 104.72537071590156\n",
      "7-th epoch val loss 104.60703295794404\n",
      "8-th epoch train loss 101.46796767470316\n",
      "8-th epoch val loss 101.35200620445073\n",
      "9-th epoch train loss 98.31234925377113\n",
      "9-th epoch val loss 98.19870484986218\n",
      "10-th epoch train loss 95.25534258176717\n",
      "10-th epoch val loss 95.14395763448802\n",
      "11-th epoch train loss 92.2938735746976\n",
      "11-th epoch val loss 92.18469203946862\n",
      "12-th epoch train loss 89.42496386202501\n",
      "12-th epoch val loss 89.31793121427687\n",
      "13-th epoch train loss 86.64572780839858\n",
      "13-th epoch val loss 86.54079099979607\n",
      "14-th epoch train loss 83.9533696280292\n",
      "14-th epoch val loss 83.85047704400164\n",
      "15-th epoch train loss 81.34518058882799\n",
      "15-th epoch val loss 81.2442820073665\n",
      "16-th epoch train loss 78.8185363035163\n",
      "16-th epoch val loss 78.71958285519932\n",
      "17-th epoch train loss 76.37089410500231\n",
      "17-th epoch val loss 76.27383823421198\n",
      "18-th epoch train loss 73.99979050340308\n",
      "18-th epoch val loss 73.90458593069641\n",
      "19-th epoch train loss 71.70283872217297\n",
      "19-th epoch val loss 71.60944040777248\n",
      "20-th epoch train loss 69.47772631087791\n",
      "20-th epoch val loss 69.38609041924788\n",
      "21-th epoch train loss 67.3222128322316\n",
      "21-th epoch val loss 67.23229669770696\n",
      "22-th epoch train loss 65.23412762108396\n",
      "22-th epoch val loss 65.1458897145197\n",
      "23-th epoch train loss 63.21136761312375\n",
      "23-th epoch val loss 63.12476750953405\n",
      "24-th epoch train loss 61.25189524112745\n",
      "24-th epoch val loss 61.16689358828436\n",
      "25-th epoch train loss 59.35373639665299\n",
      "25-th epoch val loss 59.27029488461563\n",
      "26-th epoch train loss 57.514978455143186\n",
      "26-th epoch val loss 57.43305978668926\n",
      "27-th epoch train loss 55.73376836246656\n",
      "27-th epoch val loss 55.65333622439872\n",
      "28-th epoch train loss 54.00831078098445\n",
      "28-th epoch val loss 53.92932981628516\n",
      "29-th epoch train loss 52.336866293293205\n",
      "29-th epoch val loss 52.2593020741023\n",
      "30-th epoch train loss 50.71774966184747\n",
      "30-th epoch val loss 50.64156866323758\n",
      "31-th epoch train loss 49.14932814272644\n",
      "31-th epoch val loss 49.07449771725197\n",
      "32-th epoch train loss 47.63001985185894\n",
      "32-th epoch val loss 47.556508204855504\n",
      "33-th epoch train loss 46.15829218207603\n",
      "33-th epoch val loss 46.08606834768729\n",
      "34-th epoch train loss 44.73266026940975\n",
      "34-th epoch val loss 44.66169408731997\n",
      "35-th epoch train loss 43.35168550710653\n",
      "35-th epoch val loss 43.281947599957384\n",
      "36-th epoch train loss 42.01397410587119\n",
      "36-th epoch val loss 41.945435857342105\n",
      "37-th epoch train loss 40.71817569890324\n",
      "37-th epoch val loss 40.65080923243546\n",
      "38-th epoch train loss 39.46298199033272\n",
      "38-th epoch val loss 39.396760148477476\n",
      "39-th epoch train loss 38.24712544570528\n",
      "39-th epoch val loss 38.18202177007722\n",
      "40-th epoch train loss 37.06937802320873\n",
      "40-th epoch val loss 37.00536673502659\n",
      "41-th epoch train loss 35.92854994437383\n",
      "41-th epoch val loss 35.86560592557044\n",
      "42-th epoch train loss 34.82348850302146\n",
      "42-th epoch val loss 34.76158727790599\n",
      "43-th epoch train loss 33.75307691126654\n",
      "43-th epoch val loss 33.69219462872242\n",
      "44-th epoch train loss 32.71623318142594\n",
      "44-th epoch val loss 32.656346597628335\n",
      "45-th epoch train loss 31.71190904271383\n",
      "45-th epoch val loss 31.652995504350944\n",
      "46-th epoch train loss 30.739088891642123\n",
      "46-th epoch val loss 30.6811263196253\n",
      "47-th epoch train loss 29.796788775077765\n",
      "47-th epoch val loss 29.739755648725716\n",
      "48-th epoch train loss 28.884055404941073\n",
      "48-th epoch val loss 28.827930746623853\n",
      "49-th epoch train loss 27.99996520356085\n",
      "49-th epoch val loss 27.944728563789923\n",
      "50-th epoch train loss 27.143623378732602\n",
      "50-th epoch val loss 27.089254821683607\n",
      "51-th epoch train loss 26.31416302755615\n",
      "51-th epoch val loss 26.26064311701131\n",
      "52-th epoch train loss 25.510744268157175\n",
      "52-th epoch val loss 25.45805405385487\n",
      "53-th epoch train loss 24.732553398425495\n",
      "53-th epoch val loss 24.680674402804744\n",
      "54-th epoch train loss 23.978802080929597\n",
      "54-th epoch val loss 23.92771628625769\n",
      "55-th epoch train loss 23.24872655319327\n",
      "55-th epoch val loss 23.198416389065002\n",
      "56-th epoch train loss 22.541586862545227\n",
      "56-th epoch val loss 22.492035193742762\n",
      "57-th epoch train loss 21.85666612477761\n",
      "57-th epoch val loss 21.80785623948006\n",
      "58-th epoch train loss 21.19326980587249\n",
      "58-th epoch val loss 21.145185404204845\n",
      "59-th epoch train loss 20.55072502607894\n",
      "59-th epoch val loss 20.503350208990238\n",
      "60-th epoch train loss 19.92837988564536\n",
      "60-th epoch val loss 19.881699144106232\n",
      "61-th epoch train loss 19.325602811533464\n",
      "61-th epoch val loss 19.279601016043554\n",
      "62-th epoch train loss 18.741781924461183\n",
      "62-th epoch val loss 18.696444314857217\n",
      "63-th epoch train loss 18.176324425642097\n",
      "63-th epoch val loss 18.13163660119771\n",
      "64-th epoch train loss 17.628656002608807\n",
      "64-th epoch val loss 17.584603912417293\n",
      "65-th epoch train loss 17.098220253526375\n",
      "65-th epoch val loss 17.054790187158158\n",
      "66-th epoch train loss 16.584478129420834\n",
      "66-th epoch val loss 16.54165670784733\n",
      "67-th epoch train loss 16.086907393765323\n",
      "67-th epoch val loss 16.04468156054139\n",
      "68-th epoch train loss 15.60500209888396\n",
      "68-th epoch val loss 15.563359111581208\n",
      "69-th epoch train loss 15.138272078650248\n",
      "69-th epoch val loss 15.09719950053379\n",
      "70-th epoch train loss 14.686242456973096\n",
      "70-th epoch val loss 14.645728148914552\n",
      "71-th epoch train loss 14.248453171579317\n",
      "71-th epoch val loss 14.208485284199064\n",
      "72-th epoch train loss 13.824458512616792\n",
      "72-th epoch val loss 13.785025478648643\n",
      "73-th epoch train loss 13.413826675617154\n",
      "73-th epoch val loss 13.374917202488957\n",
      "74-th epoch train loss 13.016139328371347\n",
      "74-th epoch val loss 12.977742390995084\n",
      "75-th epoch train loss 12.63099119128518\n",
      "75-th epoch val loss 12.593096025050384\n",
      "76-th epoch train loss 12.257989630795558\n",
      "76-th epoch val loss 12.22058572476008\n",
      "77-th epoch train loss 11.896754265441004\n",
      "77-th epoch val loss 11.859831355713293\n",
      "78-th epoch train loss 11.546916584192896\n",
      "78-th epoch val loss 11.51046464750015\n",
      "79-th epoch train loss 11.208119576665862\n",
      "79-th epoch val loss 11.172128824102597\n",
      "80-th epoch train loss 10.880017374837841\n",
      "80-th epoch val loss 10.84447824578958\n",
      "81-th epoch train loss 10.562274905921685\n",
      "81-th epoch val loss 10.52717806215858\n",
      "82-th epoch train loss 10.254567556041424\n",
      "82-th epoch val loss 10.219903875976893\n",
      "83-th epoch train loss 9.956580844376958\n",
      "83-th epoch val loss 9.922341417486443\n",
      "84-th epoch train loss 9.668010107451586\n",
      "84-th epoch val loss 9.63418622884675\n",
      "85-th epoch train loss 9.388560193246697\n",
      "85-th epoch val loss 9.355143358400545\n",
      "86-th epoch train loss 9.11794516483796\n",
      "86-th epoch val loss 9.084927064456426\n",
      "87-th epoch train loss 8.855888013256761\n",
      "87-th epoch val loss 8.8232605282925\n",
      "88-th epoch train loss 8.602120379289838\n",
      "88-th epoch val loss 8.569875576094088\n",
      "89-th epoch train loss 8.356382283939032\n",
      "89-th epoch val loss 8.32451240954748\n",
      "90-th epoch train loss 8.11842186727172\n",
      "90-th epoch val loss 8.08691934482051\n",
      "91-th epoch train loss 7.8879951354008595\n",
      "91-th epoch val loss 7.856852559668915\n",
      "92-th epoch train loss 7.664865715341704\n",
      "92-th epoch val loss 7.634075848415733\n",
      "93-th epoch train loss 7.448804617500095\n",
      "93-th epoch val loss 7.418360384558661\n",
      "94-th epoch train loss 7.23959000555493\n",
      "94-th epoch val loss 7.209484490768182\n",
      "95-th epoch train loss 7.037006973504659\n",
      "95-th epoch val loss 7.007233416046342\n",
      "96-th epoch train loss 6.840847329655007\n",
      "96-th epoch val loss 6.811399119823486\n",
      "97-th epoch train loss 6.650909387331827\n",
      "97-th epoch val loss 6.6217800627770105\n",
      "98-th epoch train loss 6.466997762109949\n",
      "98-th epoch val loss 6.438181004163004\n",
      "99-th epoch train loss 6.288923175355156\n",
      "99-th epoch val loss 6.2604128054580865\n",
      "100-th epoch train loss 6.116502263882973\n",
      "100-th epoch val loss 6.088292240115142\n",
      "101-th epoch train loss 5.949557395543806\n",
      "101-th epoch val loss 5.921641809242643\n",
      "102-th epoch train loss 5.787916490550139\n",
      "102-th epoch val loss 5.760289563023291\n",
      "103-th epoch train loss 5.63141284836703\n",
      "103-th epoch val loss 5.604068927693345\n",
      "104-th epoch train loss 5.479884979992839\n",
      "104-th epoch val loss 5.452818537909626\n",
      "105-th epoch train loss 5.333176445462451\n",
      "105-th epoch val loss 5.306382074336532\n",
      "106-th epoch train loss 5.191135696410448\n",
      "106-th epoch val loss 5.16460810629061\n",
      "107-th epoch train loss 5.053615923536833\n",
      "107-th epoch val loss 5.02734993928535\n",
      "108-th epoch train loss 4.920474908822675\n",
      "108-th epoch val loss 4.894465467323622\n",
      "109-th epoch train loss 4.791574882347929\n",
      "109-th epoch val loss 4.7658170297901155\n",
      "110-th epoch train loss 4.66678238356817\n",
      "110-th epoch val loss 4.641271272800556\n",
      "111-th epoch train loss 4.545968126911498\n",
      "111-th epoch val loss 4.520699014869019\n",
      "112-th epoch train loss 4.429006871561181\n",
      "112-th epoch val loss 4.403975116758984\n",
      "113-th epoch train loss 4.315777295293744\n",
      "113-th epoch val loss 4.2909783553879\n",
      "114-th epoch train loss 4.206161872246332\n",
      "114-th epoch val loss 4.181591301659115\n",
      "115-th epoch train loss 4.100046754491038\n",
      "115-th epoch val loss 4.075700202098954\n",
      "116-th epoch train loss 3.9973216572977344\n",
      "116-th epoch val loss 3.9731948641805244\n",
      "117-th epoch train loss 3.897879747970601\n",
      "117-th epoch val loss 3.8739685452194874\n",
      "118-th epoch train loss 3.8016175381471293\n",
      "118-th epoch val loss 3.777917844730633\n",
      "119-th epoch train loss 3.7084347794518715\n",
      "119-th epoch val loss 3.68494260013757\n",
      "120-th epoch train loss 3.6182343624004694\n",
      "120-th epoch val loss 3.594945785731139\n",
      "121-th epoch train loss 3.5309222184528566\n",
      "121-th epoch val loss 3.507833414775444\n",
      "122-th epoch train loss 3.4464072251175932\n",
      "122-th epoch val loss 3.423514444663543\n",
      "123-th epoch train loss 3.364601114012366\n",
      "123-th epoch val loss 3.3419006850278663\n",
      "124-th epoch train loss 3.2854183817886624\n",
      "124-th epoch val loss 3.262906708713405\n",
      "125-th epoch train loss 3.208776203831454\n",
      "125-th epoch val loss 3.1864497655245483\n",
      "126-th epoch train loss 3.1345943506475167\n",
      "126-th epoch val loss 3.112449698659251\n",
      "127-th epoch train loss 3.06279510685872\n",
      "127-th epoch val loss 3.040828863746872\n",
      "128-th epoch train loss 2.993303192719179\n",
      "128-th epoch val loss 2.9715120504086525\n",
      "129-th epoch train loss 2.9260456880777133\n",
      "129-th epoch val loss 2.9044264062622887\n",
      "130-th epoch train loss 2.8609519587095162\n",
      "130-th epoch val loss 2.8395013632945387\n",
      "131-th epoch train loss 2.797953584943256\n",
      "131-th epoch val loss 2.776668566528133\n",
      "132-th epoch train loss 2.7369842925121843\n",
      "132-th epoch val loss 2.7158618049115884\n",
      "133-th epoch train loss 2.6779798855599934\n",
      "133-th epoch val loss 2.657016944362698\n",
      "134-th epoch train loss 2.6208781817343767\n",
      "134-th epoch val loss 2.6000718628986825\n",
      "135-th epoch train loss 2.565618949303274\n",
      "135-th epoch val loss 2.544966387788027\n",
      "136-th epoch train loss 2.512143846230851\n",
      "136-th epoch val loss 2.4916422346610614\n",
      "137-th epoch train loss 2.4603963611521995\n",
      "137-th epoch val loss 2.440042948518313\n",
      "138-th epoch train loss 2.4103217561876598\n",
      "138-th epoch val loss 2.390113846577566\n",
      "139-th epoch train loss 2.361867011539468\n",
      "139-th epoch val loss 2.341801962902333\n",
      "140-th epoch train loss 2.314980771815258\n",
      "140-th epoch val loss 2.2950559947563267\n",
      "141-th epoch train loss 2.2696132940246665\n",
      "141-th epoch val loss 2.2498262506301803\n",
      "142-th epoch train loss 2.225716397196925\n",
      "142-th epoch val loss 2.2060645998883337\n",
      "143-th epoch train loss 2.1832434135689867\n",
      "143-th epoch val loss 2.1637244239856592\n",
      "144-th epoch train loss 2.1421491412952913\n",
      "144-th epoch val loss 2.1227605692049516\n",
      "145-th epoch train loss 2.1023897986317746\n",
      "145-th epoch val loss 2.0831293008679057\n",
      "146-th epoch train loss 2.0639229795482503\n",
      "146-th epoch val loss 2.044788258973746\n",
      "147-th epoch train loss 2.0267076107246464\n",
      "147-th epoch val loss 2.0076964152209884\n",
      "148-th epoch train loss 1.9907039098880555\n",
      "148-th epoch val loss 1.9718140313693353\n",
      "149-th epoch train loss 1.955873345448807\n",
      "149-th epoch val loss 1.937102618899926\n",
      "150-th epoch train loss 1.9221785973951404\n",
      "150-th epoch val loss 1.9035248999335255\n",
      "151-th epoch train loss 1.8895835194072597\n",
      "151-th epoch val loss 1.8710447693674754\n",
      "152-th epoch train loss 1.8580531021528341\n",
      "152-th epoch val loss 1.8396272581934758\n",
      "153-th epoch train loss 1.8275534377271205\n",
      "153-th epoch val loss 1.8092384979593858\n",
      "154-th epoch train loss 1.7980516852020783\n",
      "154-th epoch val loss 1.7798456863394438\n",
      "155-th epoch train loss 1.7695160372499377\n",
      "155-th epoch val loss 1.7514170537783798\n",
      "156-th epoch train loss 1.7419156878077589\n",
      "156-th epoch val loss 1.7239218311759659\n",
      "157-th epoch train loss 1.7152208007505634\n",
      "157-th epoch val loss 1.6973302185796182\n",
      "158-th epoch train loss 1.6894024795416314\n",
      "158-th epoch val loss 1.6716133548536354\n",
      "159-th epoch train loss 1.6644327378295156\n",
      "159-th epoch val loss 1.6467432882946669\n",
      "160-th epoch train loss 1.6402844709623026\n",
      "160-th epoch val loss 1.6226929481639323\n",
      "161-th epoch train loss 1.6169314283905412\n",
      "161-th epoch val loss 1.5994361171076383\n",
      "162-th epoch train loss 1.594348186931168\n",
      "162-th epoch val loss 1.5769474044379317\n",
      "163-th epoch train loss 1.5725101248656008\n",
      "163-th epoch val loss 1.5552022202475757\n",
      "164-th epoch train loss 1.5513933968460212\n",
      "164-th epoch val loss 1.5341767503323782\n",
      "165-th epoch train loss 1.5309749095846799\n",
      "165-th epoch val loss 1.5138479318962244\n",
      "166-th epoch train loss 1.5112322983018125\n",
      "166-th epoch val loss 1.4941934300143065\n",
      "167-th epoch train loss 1.4921439039085602\n",
      "167-th epoch val loss 1.4751916148309607\n",
      "168-th epoch train loss 1.4736887509019727\n",
      "168-th epoch val loss 1.456821539469201\n",
      "169-th epoch train loss 1.4558465259499285\n",
      "169-th epoch val loss 1.4390629186297936\n",
      "170-th epoch train loss 1.4385975571444718\n",
      "170-th epoch val loss 1.4218961078583765\n",
      "171-th epoch train loss 1.421922793902737\n",
      "171-th epoch val loss 1.4053020834598196\n",
      "172-th epoch train loss 1.4058037874952938\n",
      "172-th epoch val loss 1.3892624230396518\n",
      "173-th epoch train loss 1.3902226721823556\n",
      "173-th epoch val loss 1.3737592866530226\n",
      "174-th epoch train loss 1.3751621469389217\n",
      "174-th epoch val loss 1.3587753985422615\n",
      "175-th epoch train loss 1.3606054577504871\n",
      "175-th epoch val loss 1.3442940294446961\n",
      "176-th epoch train loss 1.3465363804615604\n",
      "176-th epoch val loss 1.3302989794529627\n",
      "177-th epoch train loss 1.3329392041597499\n",
      "177-th epoch val loss 1.316774561410591\n",
      "178-th epoch train loss 1.3197987150787296\n",
      "178-th epoch val loss 1.3037055848261694\n",
      "179-th epoch train loss 1.3071001810039269\n",
      "179-th epoch val loss 1.2910773402899536\n",
      "180-th epoch train loss 1.2948293361652474\n",
      "180-th epoch val loss 1.2788755843772324\n",
      "181-th epoch train loss 1.2829723666016752\n",
      "181-th epoch val loss 1.2670865250233043\n",
      "182-th epoch train loss 1.2715158959830317\n",
      "182-th epoch val loss 1.2556968073553472\n",
      "183-th epoch train loss 1.2604469718746454\n",
      "183-th epoch val loss 1.2446934999669454\n",
      "184-th epoch train loss 1.249753052431134\n",
      "184-th epoch val loss 1.2340640816214883\n",
      "185-th epoch train loss 1.2394219935059212\n",
      "185-th epoch val loss 1.223796428371051\n",
      "186-th epoch train loss 1.2294420361635208\n",
      "186-th epoch val loss 1.2138788010778216\n",
      "187-th epoch train loss 1.219801794582044\n",
      "187-th epoch val loss 1.204299833325516\n",
      "188-th epoch train loss 1.2104902443337702\n",
      "188-th epoch val loss 1.1950485197086347\n",
      "189-th epoch train loss 1.2014967110319703\n",
      "189-th epoch val loss 1.1861142044877644\n",
      "190-th epoch train loss 1.1928108593325972\n",
      "190-th epoch val loss 1.1774865705995277\n",
      "191-th epoch train loss 1.1844226822797546\n",
      "191-th epoch val loss 1.1691556290101142\n",
      "192-th epoch train loss 1.1763224909842382\n",
      "192-th epoch val loss 1.161111708401682\n",
      "193-th epoch train loss 1.1685009046247619\n",
      "193-th epoch val loss 1.153345445181254\n",
      "194-th epoch train loss 1.160948840761808\n",
      "194-th epoch val loss 1.1458477738020483\n",
      "195-th epoch train loss 1.1536575059543464\n",
      "195-th epoch val loss 1.138609917387495\n",
      "196-th epoch train loss 1.1466183866699922\n",
      "196-th epoch val loss 1.1316233786485161\n",
      "197-th epoch train loss 1.1398232404794384\n",
      "197-th epoch val loss 1.1248799310849042\n",
      "198-th epoch train loss 1.1332640875263014\n",
      "198-th epoch val loss 1.1183716104619468\n",
      "199-th epoch train loss 1.1269332022637963\n",
      "199-th epoch val loss 1.1120907065537189\n",
      "200-th epoch train loss 1.1208231054499112\n",
      "200-th epoch val loss 1.1060297551447105\n",
      "201-th epoch train loss 1.1149265563930206\n",
      "201-th epoch val loss 1.1001815302817408\n",
      "202-th epoch train loss 1.109236545440131\n",
      "202-th epoch val loss 1.0945390367683538\n",
      "203-th epoch train loss 1.1037462867001824\n",
      "203-th epoch val loss 1.0890955028941225\n",
      "204-th epoch train loss 1.0984492109950819\n",
      "204-th epoch val loss 1.0838443733915357\n",
      "205-th epoch train loss 1.0933389590313536\n",
      "205-th epoch val loss 1.0787793026133752\n",
      "206-th epoch train loss 1.0884093747855346\n",
      "206-th epoch val loss 1.0738941479236879\n",
      "207-th epoch train loss 1.0836544990966392\n",
      "207-th epoch val loss 1.069182963295704\n",
      "208-th epoch train loss 1.0790685634592343\n",
      "208-th epoch val loss 1.0646399931102342\n",
      "209-th epoch train loss 1.074645984010866\n",
      "209-th epoch val loss 1.0602596661482921\n",
      "210-th epoch train loss 1.0703813557077706\n",
      "210-th epoch val loss 1.0560365897718806\n",
      "211-th epoch train loss 1.066269446682999\n",
      "211-th epoch val loss 1.0519655442870701\n",
      "212-th epoch train loss 1.0623051927812566\n",
      "212-th epoch val loss 1.0480414774836797\n",
      "213-th epoch train loss 1.0584836922649459\n",
      "213-th epoch val loss 1.0442594993460448\n",
      "214-th epoch train loss 1.0548002006860682\n",
      "214-th epoch val loss 1.0406148769295354\n",
      "215-th epoch train loss 1.0512501259188078\n",
      "215-th epoch val loss 1.0371030293976524\n",
      "216-th epoch train loss 1.0478290233477832\n",
      "216-th epoch val loss 1.0337195232146783\n",
      "217-th epoch train loss 1.0445325912070982\n",
      "217-th epoch val loss 1.030460067489037\n",
      "218-th epoch train loss 1.0413566660655031\n",
      "218-th epoch val loss 1.0273205094626576\n",
      "219-th epoch train loss 1.0382972184530816\n",
      "219-th epoch val loss 1.0242968301417663\n",
      "220-th epoch train loss 1.0353503486250644\n",
      "220-th epoch val loss 1.0213851400647183\n",
      "221-th epoch train loss 1.0325122824584736\n",
      "221-th epoch val loss 1.0185816752025594\n",
      "222-th epoch train loss 1.0297793674774567\n",
      "222-th epoch val loss 1.0158827929881966\n",
      "223-th epoch train loss 1.0271480690032915\n",
      "223-th epoch val loss 1.0132849684701446\n",
      "224-th epoch train loss 1.0246149664251674\n",
      "224-th epoch val loss 1.0107847905869713\n",
      "225-th epoch train loss 1.022176749587963\n",
      "225-th epoch val loss 1.0083789585586502\n",
      "226-th epoch train loss 1.0198302152933847\n",
      "226-th epoch val loss 1.0060642783911988\n",
      "227-th epoch train loss 1.017572263910903\n",
      "227-th epoch val loss 1.0038376594910317\n",
      "228-th epoch train loss 1.0153998960950719\n",
      "228-th epoch val loss 1.0016961113856218\n",
      "229-th epoch train loss 1.0133102096058983\n",
      "229-th epoch val loss 0.9996367405471378\n",
      "230-th epoch train loss 1.011300396229051\n",
      "230-th epoch val loss 0.9976567473158473\n",
      "231-th epoch train loss 1.0093677387927766\n",
      "231-th epoch val loss 0.9957534229201562\n",
      "232-th epoch train loss 1.007509608278518\n",
      "232-th epoch val loss 0.9939241465902796\n",
      "233-th epoch train loss 1.0057234610222858\n",
      "233-th epoch val loss 0.9921663827626017\n",
      "234-th epoch train loss 1.0040068360039651\n",
      "234-th epoch val loss 0.9904776783719019\n",
      "235-th epoch train loss 1.0023573522218\n",
      "235-th epoch val loss 0.9888556602286963\n",
      "236-th epoch train loss 1.0007727061493903\n",
      "236-th epoch val loss 0.9872980324790268\n",
      "237-th epoch train loss 0.9992506692726297\n",
      "237-th epoch val loss 0.9858025741441323\n",
      "238-th epoch train loss 0.9977890857040788\n",
      "238-th epoch val loss 0.9843671367374954\n",
      "239-th epoch train loss 0.9963858698723546\n",
      "239-th epoch val loss 0.9829896419568448\n",
      "240-th epoch train loss 0.9950390042841918\n",
      "240-th epoch val loss 0.9816680794487754\n",
      "241-th epoch train loss 0.9937465373568996\n",
      "241-th epoch val loss 0.9804005046437076\n",
      "242-th epoch train loss 0.9925065813190149\n",
      "242-th epoch val loss 0.9791850366589893\n",
      "243-th epoch train loss 0.9913173101770217\n",
      "243-th epoch val loss 0.9780198562680089\n",
      "244-th epoch train loss 0.9901769577460604\n",
      "244-th epoch val loss 0.9769032039332506\n",
      "245-th epoch train loss 0.9890838157426378\n",
      "245-th epoch val loss 0.9758333779012985\n",
      "246-th epoch train loss 0.9880362319373861\n",
      "246-th epoch val loss 0.974808732357838\n",
      "247-th epoch train loss 0.9870326083660016\n",
      "247-th epoch val loss 0.9738276756407972\n",
      "248-th epoch train loss 0.9860713995965336\n",
      "248-th epoch val loss 0.9728886685097783\n",
      "249-th epoch train loss 0.9851511110512715\n",
      "249-th epoch val loss 0.9719902224700563\n",
      "250-th epoch train loss 0.9842702973815033\n",
      "250-th epoch val loss 0.9711308981493936\n",
      "251-th epoch train loss 0.9834275608935137\n",
      "251-th epoch val loss 0.970309303726058\n",
      "252-th epoch train loss 0.9826215500241915\n",
      "252-th epoch val loss 0.9695240934064023\n",
      "253-th epoch train loss 0.981850957864721\n",
      "253-th epoch val loss 0.9687739659504886\n",
      "254-th epoch train loss 0.9811145207308243\n",
      "254-th epoch val loss 0.9680576632442255\n",
      "255-th epoch train loss 0.9804110167781175\n",
      "255-th epoch val loss 0.9673739689165749\n",
      "256-th epoch train loss 0.9797392646611568\n",
      "256-th epoch val loss 0.9667217070004142\n",
      "257-th epoch train loss 0.9790981222348121\n",
      "257-th epoch val loss 0.9660997406356846\n",
      "258-th epoch train loss 0.9784864852966316\n",
      "258-th epoch val loss 0.9655069708135001\n",
      "259-th epoch train loss 0.977903286368932\n",
      "259-th epoch val loss 0.9649423351599358\n",
      "260-th epoch train loss 0.9773474935193449\n",
      "260-th epoch val loss 0.964404806758249\n",
      "261-th epoch train loss 0.9768181092186369\n",
      "261-th epoch val loss 0.9638933930083297\n",
      "262-th epoch train loss 0.9763141692346228\n",
      "262-th epoch val loss 0.9634071345222178\n",
      "263-th epoch train loss 0.9758347415610398\n",
      "263-th epoch val loss 0.9629451040545444\n",
      "264-th epoch train loss 0.9753789253802934\n",
      "264-th epoch val loss 0.9625064054668151\n",
      "265-th epoch train loss 0.9749458500590081\n",
      "265-th epoch val loss 0.9620901727244645\n",
      "266-th epoch train loss 0.9745346741753564\n",
      "266-th epoch val loss 0.961695568925661\n",
      "267-th epoch train loss 0.9741445845771644\n",
      "267-th epoch val loss 0.9613217853608582\n",
      "268-th epoch train loss 0.97377479546984\n",
      "268-th epoch val loss 0.9609680406021366\n",
      "269-th epoch train loss 0.9734245475331735\n",
      "269-th epoch val loss 0.9606335796213922\n",
      "270-th epoch train loss 0.9730931070661148\n",
      "270-th epoch val loss 0.9603176729364753\n",
      "271-th epoch train loss 0.9727797651586462\n",
      "271-th epoch val loss 0.9600196157843912\n",
      "272-th epoch train loss 0.9724838368899019\n",
      "272-th epoch val loss 0.9597387273207281\n",
      "273-th epoch train loss 0.9722046605517041\n",
      "273-th epoch val loss 0.9594743498444726\n",
      "274-th epoch train loss 0.9719415968967277\n",
      "274-th epoch val loss 0.959225848047427\n",
      "275-th epoch train loss 0.9716940284105109\n",
      "275-th epoch val loss 0.958992608287451\n",
      "276-th epoch train loss 0.9714613586065706\n",
      "276-th epoch val loss 0.958774037884779\n",
      "277-th epoch train loss 0.9712430113438882\n",
      "277-th epoch val loss 0.9585695644406901\n",
      "278-th epoch train loss 0.9710384301660737\n",
      "278-th epoch val loss 0.9583786351778266\n",
      "279-th epoch train loss 0.9708470776615155\n",
      "279-th epoch val loss 0.9582007163014832\n",
      "280-th epoch train loss 0.9706684348438658\n",
      "280-th epoch val loss 0.9580352923812001\n",
      "281-th epoch train loss 0.9705020005522174\n",
      "281-th epoch val loss 0.9578818657520346\n",
      "282-th epoch train loss 0.9703472908703529\n",
      "282-th epoch val loss 0.9577399559348785\n",
      "283-th epoch train loss 0.9702038385644711\n",
      "283-th epoch val loss 0.9576090990752332\n",
      "284-th epoch train loss 0.9700711925388023\n",
      "284-th epoch val loss 0.957488847399854\n",
      "285-th epoch train loss 0.9699489173085563\n",
      "285-th epoch val loss 0.9573787686907058\n",
      "286-th epoch train loss 0.969836592489654\n",
      "286-th epoch val loss 0.9572784457756851\n",
      "287-th epoch train loss 0.9697338123047163\n",
      "287-th epoch val loss 0.9571874760355786\n",
      "288-th epoch train loss 0.9696401851047981\n",
      "288-th epoch val loss 0.9571054709267466\n",
      "289-th epoch train loss 0.9695553329063717\n",
      "289-th epoch val loss 0.9570320555190426\n",
      "290-th epoch train loss 0.9694788909430788\n",
      "290-th epoch val loss 0.9569668680484773\n",
      "291-th epoch train loss 0.9694105072317875\n",
      "291-th epoch val loss 0.9569095594841767\n",
      "292-th epoch train loss 0.9693498421525034\n",
      "292-th epoch val loss 0.9568597931091698\n",
      "293-th epoch train loss 0.9692965680416977\n",
      "293-th epoch val loss 0.9568172441145838\n",
      "294-th epoch train loss 0.9692503687986288\n",
      "294-th epoch val loss 0.9567815992068109\n",
      "295-th epoch train loss 0.9692109395042476\n",
      "295-th epoch val loss 0.9567525562272473\n",
      "296-th epoch train loss 0.969177986052297\n",
      "296-th epoch val loss 0.9567298237842062\n",
      "297-th epoch train loss 0.9691512247922095\n",
      "297-th epoch val loss 0.9567131208966148\n",
      "298-th epoch train loss 0.9691303821834352\n",
      "298-th epoch val loss 0.9567021766491313\n",
      "299-th epoch train loss 0.9691151944608489\n",
      "299-th epoch val loss 0.9566967298583179\n",
      "300-th epoch train loss 0.9691054073108699\n",
      "300-th epoch val loss 0.9566965287495205\n",
      "301-th epoch train loss 0.9691007755579709\n",
      "301-th epoch val loss 0.9567013306441156\n",
      "302-th epoch train loss 0.9691010628612384\n",
      "302-th epoch val loss 0.9567109016568026\n",
      "303-th epoch train loss 0.9691060414206762\n",
      "303-th epoch val loss 0.9567250164026188\n",
      "304-th epoch train loss 0.9691154916929346\n",
      "304-th epoch val loss 0.9567434577133697\n",
      "305-th epoch train loss 0.9691292021161745\n",
      "305-th epoch val loss 0.9567660163631831\n",
      "306-th epoch train loss 0.969146968843773\n",
      "306-th epoch val loss 0.9567924908028891\n",
      "307-th epoch train loss 0.9691685954865992\n",
      "307-th epoch val loss 0.9568226869029574\n",
      "308-th epoch train loss 0.9691938928635774\n",
      "308-th epoch val loss 0.9568564177047134\n",
      "309-th epoch train loss 0.9692226787602831\n",
      "309-th epoch val loss 0.9568935031795722\n",
      "310-th epoch train loss 0.9692547776953208\n",
      "310-th epoch val loss 0.9569337699960412\n",
      "311-th epoch train loss 0.9692900206942253\n",
      "311-th epoch val loss 0.95697705129424\n",
      "312-th epoch train loss 0.9693282450706653\n",
      "312-th epoch val loss 0.9570231864677058\n",
      "313-th epoch train loss 0.9693692942146989\n",
      "313-th epoch val loss 0.9570720209522428\n",
      "314-th epoch train loss 0.9694130173878749\n",
      "314-th epoch val loss 0.957123406021606\n",
      "315-th epoch train loss 0.9694592695249529\n",
      "315-th epoch val loss 0.9571771985897924\n",
      "316-th epoch train loss 0.969507911042037\n",
      "316-th epoch val loss 0.9572332610197365\n",
      "317-th epoch train loss 0.969558807650913\n",
      "317-th epoch val loss 0.9572914609381978\n",
      "318-th epoch train loss 0.9696118301794043\n",
      "318-th epoch val loss 0.9573516710566601\n",
      "319-th epoch train loss 0.9696668543975459\n",
      "319-th epoch val loss 0.9574137689980313\n",
      "320-th epoch train loss 0.9697237608493917\n",
      "320-th epoch val loss 0.9574776371289808\n",
      "321-th epoch train loss 0.9697824346902844\n",
      "321-th epoch val loss 0.9575431623977144\n",
      "322-th epoch train loss 0.9698427655294078\n",
      "322-th epoch val loss 0.9576102361770324\n",
      "323-th epoch train loss 0.9699046472774514\n",
      "323-th epoch val loss 0.9576787541124896\n",
      "324-th epoch train loss 0.969967977999239\n",
      "324-th epoch val loss 0.9577486159755073\n",
      "325-th epoch train loss 0.9700326597711384\n",
      "325-th epoch val loss 0.9578197255212595\n",
      "326-th epoch train loss 0.970098598543133\n",
      "326-th epoch val loss 0.9578919903512144\n",
      "327-th epoch train loss 0.9701657040053719\n",
      "327-th epoch val loss 0.9579653217801425\n",
      "328-th epoch train loss 0.9702338894590872\n",
      "328-th epoch val loss 0.9580396347074854\n",
      "329-th epoch train loss 0.9703030716917174\n",
      "329-th epoch val loss 0.9581148474929237\n",
      "330-th epoch train loss 0.9703731708561155\n",
      "330-th epoch val loss 0.9581908818360205\n",
      "331-th epoch train loss 0.9704441103537047\n",
      "331-th epoch val loss 0.9582676626598037\n",
      "332-th epoch train loss 0.9705158167214596\n",
      "332-th epoch val loss 0.9583451179981691\n",
      "333-th epoch train loss 0.9705882195225913\n",
      "333-th epoch val loss 0.9584231788869788\n",
      "334-th epoch train loss 0.9706612512408156\n",
      "334-th epoch val loss 0.9585017792587321\n",
      "335-th epoch train loss 0.9707348471780912\n",
      "335-th epoch val loss 0.958580855840706\n",
      "336-th epoch train loss 0.9708089453557159\n",
      "336-th epoch val loss 0.9586603480564394\n",
      "337-th epoch train loss 0.9708834864186814\n",
      "337-th epoch val loss 0.9587401979304753\n",
      "338-th epoch train loss 0.9709584135431659\n",
      "338-th epoch val loss 0.9588203499962299\n",
      "339-th epoch train loss 0.9710336723470844\n",
      "339-th epoch val loss 0.9589007512069138\n",
      "340-th epoch train loss 0.9711092108035806\n",
      "340-th epoch val loss 0.95898135084939\n",
      "341-th epoch train loss 0.9711849791573819\n",
      "341-th epoch val loss 0.9590621004608856\n",
      "342-th epoch train loss 0.9712609298439135\n",
      "342-th epoch val loss 0.959142953748459\n",
      "343-th epoch train loss 0.9713370174110906\n",
      "343-th epoch val loss 0.9592238665111393\n",
      "344-th epoch train loss 0.9714131984436994\n",
      "344-th epoch val loss 0.9593047965646445\n",
      "345-th epoch train loss 0.9714894314902867\n",
      "345-th epoch val loss 0.9593857036686072\n",
      "346-th epoch train loss 0.9715656769924764\n",
      "346-th epoch val loss 0.9594665494562199\n",
      "347-th epoch train loss 0.971641897216632\n",
      "347-th epoch val loss 0.9595472973662179\n",
      "348-th epoch train loss 0.9717180561877944\n",
      "348-th epoch val loss 0.9596279125771365\n",
      "349-th epoch train loss 0.9717941196258227\n",
      "349-th epoch val loss 0.9597083619437613\n",
      "350-th epoch train loss 0.9718700548836577\n",
      "350-th epoch val loss 0.9597886139356994\n",
      "351-th epoch train loss 0.9719458308876547\n",
      "351-th epoch val loss 0.9598686385780134\n",
      "352-th epoch train loss 0.9720214180799053\n",
      "352-th epoch val loss 0.9599484073938371\n",
      "353-th epoch train loss 0.9720967883624917\n",
      "353-th epoch val loss 0.9600278933489221\n",
      "354-th epoch train loss 0.9721719150436116\n",
      "354-th epoch val loss 0.9601070707980482\n",
      "355-th epoch train loss 0.972246772785506\n",
      "355-th epoch val loss 0.960185915433231\n",
      "356-th epoch train loss 0.9723213375541447\n",
      "356-th epoch val loss 0.9602644042336842\n",
      "357-th epoch train loss 0.9723955865705963\n",
      "357-th epoch val loss 0.960342515417462\n",
      "358-th epoch train loss 0.9724694982640456\n",
      "358-th epoch val loss 0.9604202283947403\n",
      "359-th epoch train loss 0.9725430522263898\n",
      "359-th epoch val loss 0.9604975237226767\n",
      "360-th epoch train loss 0.972616229168374\n",
      "360-th epoch val loss 0.9605743830618048\n",
      "361-th epoch train loss 0.9726890108772084\n",
      "361-th epoch val loss 0.9606507891339034\n",
      "362-th epoch train loss 0.9727613801756267\n",
      "362-th epoch val loss 0.9607267256813018\n",
      "363-th epoch train loss 0.9728333208823283\n",
      "363-th epoch val loss 0.9608021774275711\n",
      "364-th epoch train loss 0.9729048177737727\n",
      "364-th epoch val loss 0.9608771300395553\n",
      "365-th epoch train loss 0.9729758565472705\n",
      "365-th epoch val loss 0.9609515700907006\n",
      "366-th epoch train loss 0.973046423785334\n",
      "366-th epoch val loss 0.9610254850256394\n",
      "367-th epoch train loss 0.9731165069212521\n",
      "367-th epoch val loss 0.9610988631259911\n",
      "368-th epoch train loss 0.9731860942058357\n",
      "368-th epoch val loss 0.9611716934773341\n",
      "369-th epoch train loss 0.9732551746753092\n",
      "369-th epoch val loss 0.9612439659373189\n",
      "370-th epoch train loss 0.9733237381203023\n",
      "370-th epoch val loss 0.9613156711048746\n",
      "371-th epoch train loss 0.973391775055913\n",
      "371-th epoch val loss 0.9613868002904868\n",
      "372-th epoch train loss 0.9734592766927981\n",
      "372-th epoch val loss 0.9614573454875001\n",
      "373-th epoch train loss 0.9735262349092673\n",
      "373-th epoch val loss 0.9615272993444139\n",
      "374-th epoch train loss 0.973592642224339\n",
      "374-th epoch val loss 0.9615966551381467\n",
      "375-th epoch train loss 0.973658491771737\n",
      "375-th epoch val loss 0.9616654067482265\n",
      "376-th epoch train loss 0.9737237772747831\n",
      "376-th epoch val loss 0.9617335486318856\n",
      "377-th epoch train loss 0.973788493022171\n",
      "377-th epoch val loss 0.9618010758000227\n",
      "378-th epoch train loss 0.9738526338445834\n",
      "378-th epoch val loss 0.9618679837940117\n",
      "379-th epoch train loss 0.973916195092128\n",
      "379-th epoch val loss 0.9619342686633245\n",
      "380-th epoch train loss 0.9739791726125608\n",
      "380-th epoch val loss 0.9619999269439373\n",
      "381-th epoch train loss 0.9740415627302814\n",
      "381-th epoch val loss 0.9620649556375055\n",
      "382-th epoch train loss 0.9741033622260633\n",
      "382-th epoch val loss 0.9621293521912732\n",
      "383-th epoch train loss 0.9741645683175002\n",
      "383-th epoch val loss 0.9621931144786913\n",
      "384-th epoch train loss 0.9742251786401465\n",
      "384-th epoch val loss 0.962256240780733\n",
      "385-th epoch train loss 0.9742851912293268\n",
      "385-th epoch val loss 0.9623187297678679\n",
      "386-th epoch train loss 0.9743446045025893\n",
      "386-th epoch val loss 0.9623805804826845\n",
      "387-th epoch train loss 0.9744034172427868\n",
      "387-th epoch val loss 0.9624417923231336\n",
      "388-th epoch train loss 0.9744616285817683\n",
      "388-th epoch val loss 0.9625023650263776\n",
      "389-th epoch train loss 0.9745192379846427\n",
      "389-th epoch val loss 0.962562298653217\n",
      "390-th epoch train loss 0.9745762452346213\n",
      "390-th epoch val loss 0.9626215935730835\n",
      "391-th epoch train loss 0.9746326504183982\n",
      "391-th epoch val loss 0.9626802504495753\n",
      "392-th epoch train loss 0.9746884539120655\n",
      "392-th epoch val loss 0.9627382702265211\n",
      "393-th epoch train loss 0.9747436563675328\n",
      "393-th epoch val loss 0.9627956541145489\n",
      "394-th epoch train loss 0.9747982586994471\n",
      "394-th epoch val loss 0.9628524035781489\n",
      "395-th epoch train loss 0.9748522620725854\n",
      "395-th epoch val loss 0.9629085203232111\n",
      "396-th epoch train loss 0.9749056678897128\n",
      "396-th epoch val loss 0.9629640062850205\n",
      "397-th epoch train loss 0.9749584777798816\n",
      "397-th epoch val loss 0.9630188636166991\n",
      "398-th epoch train loss 0.9750106935871674\n",
      "398-th epoch val loss 0.9630730946780729\n",
      "399-th epoch train loss 0.9750623173598172\n",
      "399-th epoch val loss 0.9631267020249574\n",
      "400-th epoch train loss 0.9751133513398024\n",
      "400-th epoch val loss 0.9631796883988386\n",
      "401-th epoch train loss 0.9751637979527606\n",
      "401-th epoch val loss 0.9632320567169478\n",
      "402-th epoch train loss 0.9752136597983129\n",
      "402-th epoch val loss 0.9632838100627024\n",
      "403-th epoch train loss 0.9752629396407446\n",
      "403-th epoch val loss 0.9633349516765137\n",
      "404-th epoch train loss 0.9753116404000378\n",
      "404-th epoch val loss 0.9633854849469432\n",
      "405-th epoch train loss 0.9753597651432423\n",
      "405-th epoch val loss 0.9634354134021912\n",
      "406-th epoch train loss 0.9754073170761743\n",
      "406-th epoch val loss 0.9634847407019183\n",
      "407-th epoch train loss 0.9754542995354322\n",
      "407-th epoch val loss 0.9635334706293756\n",
      "408-th epoch train loss 0.975500715980717\n",
      "408-th epoch val loss 0.9635816070838416\n",
      "409-th epoch train loss 0.9755465699874497\n",
      "409-th epoch val loss 0.9636291540733515\n",
      "410-th epoch train loss 0.9755918652396736\n",
      "410-th epoch val loss 0.9636761157077129\n",
      "411-th epoch train loss 0.9756366055232297\n",
      "411-th epoch val loss 0.96372249619179\n",
      "412-th epoch train loss 0.9756807947192031\n",
      "412-th epoch val loss 0.9637682998190554\n",
      "413-th epoch train loss 0.9757244367976217\n",
      "413-th epoch val loss 0.9638135309653987\n",
      "414-th epoch train loss 0.9757675358114054\n",
      "414-th epoch val loss 0.9638581940831782\n",
      "415-th epoch train loss 0.9758100958905555\n",
      "415-th epoch val loss 0.9639022936955121\n",
      "416-th epoch train loss 0.9758521212365744\n",
      "416-th epoch val loss 0.9639458343908\n",
      "417-th epoch train loss 0.9758936161171073\n",
      "417-th epoch val loss 0.9639888208174653\n",
      "418-th epoch train loss 0.9759345848608024\n",
      "418-th epoch val loss 0.9640312576789096\n",
      "419-th epoch train loss 0.9759750318523798\n",
      "419-th epoch val loss 0.9640731497286812\n",
      "420-th epoch train loss 0.976014961527896\n",
      "420-th epoch val loss 0.964114501765833\n",
      "421-th epoch train loss 0.9760543783702096\n",
      "421-th epoch val loss 0.9641553186304794\n",
      "422-th epoch train loss 0.9760932869046274\n",
      "422-th epoch val loss 0.9641956051995372\n",
      "423-th epoch train loss 0.9761316916947379\n",
      "423-th epoch val loss 0.9642353663826456\n",
      "424-th epoch train loss 0.9761695973384129\n",
      "424-th epoch val loss 0.9642746071182601\n",
      "425-th epoch train loss 0.9762070084639799\n",
      "425-th epoch val loss 0.964313332369909\n",
      "426-th epoch train loss 0.9762439297265613\n",
      "426-th epoch val loss 0.9643515471226188\n",
      "427-th epoch train loss 0.9762803658045592\n",
      "427-th epoch val loss 0.9643892563794856\n",
      "428-th epoch train loss 0.9763163213963035\n",
      "428-th epoch val loss 0.9644264651584034\n",
      "429-th epoch train loss 0.9763518012168372\n",
      "429-th epoch val loss 0.9644631784889321\n",
      "430-th epoch train loss 0.9763868099948453\n",
      "430-th epoch val loss 0.9644994014093067\n",
      "431-th epoch train loss 0.97642135246972\n",
      "431-th epoch val loss 0.9645351389635819\n",
      "432-th epoch train loss 0.9764554333887544\n",
      "432-th epoch val loss 0.9645703961989042\n",
      "433-th epoch train loss 0.976489057504464\n",
      "433-th epoch val loss 0.9646051781629069\n",
      "434-th epoch train loss 0.9765222295720299\n",
      "434-th epoch val loss 0.9646394899012315\n",
      "435-th epoch train loss 0.9765549543468544\n",
      "435-th epoch val loss 0.9646733364551564\n",
      "436-th epoch train loss 0.9765872365822371\n",
      "436-th epoch val loss 0.9647067228593471\n",
      "437-th epoch train loss 0.976619081027155\n",
      "437-th epoch val loss 0.964739654139704\n",
      "438-th epoch train loss 0.9766504924241468\n",
      "438-th epoch val loss 0.9647721353113222\n",
      "439-th epoch train loss 0.9766814755073008\n",
      "439-th epoch val loss 0.9648041713765454\n",
      "440-th epoch train loss 0.9767120350003387\n",
      "440-th epoch val loss 0.9648357673231198\n",
      "441-th epoch train loss 0.9767421756147929\n",
      "441-th epoch val loss 0.964866928122438\n",
      "442-th epoch train loss 0.9767719020482746\n",
      "442-th epoch val loss 0.9648976587278719\n",
      "443-th epoch train loss 0.9768012189828309\n",
      "443-th epoch val loss 0.9649279640731966\n",
      "444-th epoch train loss 0.9768301310833785\n",
      "444-th epoch val loss 0.9649578490710884\n",
      "445-th epoch train loss 0.9768586429962282\n",
      "445-th epoch val loss 0.9649873186117108\n",
      "446-th epoch train loss 0.9768867593476804\n",
      "446-th epoch val loss 0.9650163775613713\n",
      "447-th epoch train loss 0.9769144847426932\n",
      "447-th epoch val loss 0.9650450307612537\n",
      "448-th epoch train loss 0.9769418237636313\n",
      "448-th epoch val loss 0.9650732830262236\n",
      "449-th epoch train loss 0.976968780969075\n",
      "449-th epoch val loss 0.9651011391436981\n",
      "450-th epoch train loss 0.9769953608926996\n",
      "450-th epoch val loss 0.9651286038725829\n",
      "451-th epoch train loss 0.9770215680422201\n",
      "451-th epoch val loss 0.9651556819422746\n",
      "452-th epoch train loss 0.9770474068983951\n",
      "452-th epoch val loss 0.9651823780517206\n",
      "453-th epoch train loss 0.9770728819140904\n",
      "453-th epoch val loss 0.9652086968685363\n",
      "454-th epoch train loss 0.977097997513401\n",
      "454-th epoch val loss 0.9652346430281842\n",
      "455-th epoch train loss 0.9771227580908247\n",
      "455-th epoch val loss 0.9652602211332006\n",
      "456-th epoch train loss 0.9771471680104918\n",
      "456-th epoch val loss 0.9652854357524768\n",
      "457-th epoch train loss 0.9771712316054445\n",
      "457-th epoch val loss 0.96531029142059\n",
      "458-th epoch train loss 0.9771949531769617\n",
      "458-th epoch val loss 0.9653347926371814\n",
      "459-th epoch train loss 0.9772183369939347\n",
      "459-th epoch val loss 0.9653589438663805\n",
      "460-th epoch train loss 0.9772413872922848\n",
      "460-th epoch val loss 0.9653827495362725\n",
      "461-th epoch train loss 0.9772641082744256\n",
      "461-th epoch val loss 0.9654062140384085\n",
      "462-th epoch train loss 0.9772865041087654\n",
      "462-th epoch val loss 0.9654293417273571\n",
      "463-th epoch train loss 0.9773085789292493\n",
      "463-th epoch val loss 0.9654521369202931\n",
      "464-th epoch train loss 0.9773303368349412\n",
      "464-th epoch val loss 0.9654746038966243\n",
      "465-th epoch train loss 0.9773517818896372\n",
      "465-th epoch val loss 0.9654967468976545\n",
      "466-th epoch train loss 0.9773729181215213\n",
      "466-th epoch val loss 0.9655185701262786\n",
      "467-th epoch train loss 0.9773937495228472\n",
      "467-th epoch val loss 0.9655400777467138\n",
      "468-th epoch train loss 0.9774142800496575\n",
      "468-th epoch val loss 0.9655612738842587\n",
      "469-th epoch train loss 0.9774345136215324\n",
      "469-th epoch val loss 0.9655821626250854\n",
      "470-th epoch train loss 0.977454454121366\n",
      "470-th epoch val loss 0.9656027480160596\n",
      "471-th epoch train loss 0.9774741053951677\n",
      "471-th epoch val loss 0.965623034064582\n",
      "472-th epoch train loss 0.9774934712519051\n",
      "472-th epoch val loss 0.9656430247384716\n",
      "473-th epoch train loss 0.9775125554633518\n",
      "473-th epoch val loss 0.9656627239658551\n",
      "474-th epoch train loss 0.9775313617639763\n",
      "474-th epoch val loss 0.9656821356350938\n",
      "475-th epoch train loss 0.9775498938508473\n",
      "475-th epoch val loss 0.9657012635947286\n",
      "476-th epoch train loss 0.977568155383564\n",
      "476-th epoch val loss 0.9657201116534477\n",
      "477-th epoch train loss 0.9775861499842036\n",
      "477-th epoch val loss 0.9657386835800728\n",
      "478-th epoch train loss 0.977603881237295\n",
      "478-th epoch val loss 0.96575698310357\n",
      "479-th epoch train loss 0.9776213526898108\n",
      "479-th epoch val loss 0.9657750139130759\n",
      "480-th epoch train loss 0.9776385678511748\n",
      "480-th epoch val loss 0.9657927796579436\n",
      "481-th epoch train loss 0.9776555301932888\n",
      "481-th epoch val loss 0.9658102839478031\n",
      "482-th epoch train loss 0.9776722431505809\n",
      "482-th epoch val loss 0.9658275303526463\n",
      "483-th epoch train loss 0.9776887101200623\n",
      "483-th epoch val loss 0.9658445224029152\n",
      "484-th epoch train loss 0.9777049344614066\n",
      "484-th epoch val loss 0.9658612635896164\n",
      "485-th epoch train loss 0.9777209194970371\n",
      "485-th epoch val loss 0.9658777573644425\n",
      "486-th epoch train loss 0.9777366685122342\n",
      "486-th epoch val loss 0.9658940071399092\n",
      "487-th epoch train loss 0.9777521847552538\n",
      "487-th epoch val loss 0.96591001628951\n",
      "488-th epoch train loss 0.9777674714374551\n",
      "488-th epoch val loss 0.9659257881478696\n",
      "489-th epoch train loss 0.977782531733446\n",
      "489-th epoch val loss 0.9659413260109271\n",
      "490-th epoch train loss 0.9777973687812355\n",
      "490-th epoch val loss 0.9659566331361121\n",
      "491-th epoch train loss 0.9778119856823981\n",
      "491-th epoch val loss 0.9659717127425466\n",
      "492-th epoch train loss 0.9778263855022501\n",
      "492-th epoch val loss 0.9659865680112447\n",
      "493-th epoch train loss 0.9778405712700311\n",
      "493-th epoch val loss 0.9660012020853246\n",
      "494-th epoch train loss 0.9778545459791008\n",
      "494-th epoch val loss 0.9660156180702368\n",
      "495-th epoch train loss 0.9778683125871367\n",
      "495-th epoch val loss 0.9660298190339877\n",
      "496-th epoch train loss 0.9778818740163494\n",
      "496-th epoch val loss 0.9660438080073824\n",
      "497-th epoch train loss 0.9778952331536952\n",
      "497-th epoch val loss 0.9660575879842652\n",
      "498-th epoch train loss 0.9779083928511028\n",
      "498-th epoch val loss 0.966071161921775\n",
      "499-th epoch train loss 0.9779213559257042\n",
      "499-th epoch val loss 0.9660845327405985\n",
      "500-th epoch train loss 0.9779341251600729\n",
      "500-th epoch val loss 0.9660977033252393\n",
      "501-th epoch train loss 0.977946703302467\n",
      "501-th epoch val loss 0.9661106765242816\n",
      "502-th epoch train loss 0.9779590930670767\n",
      "502-th epoch val loss 0.966123455150668\n",
      "503-th epoch train loss 0.9779712971342807\n",
      "503-th epoch val loss 0.9661360419819762\n",
      "504-th epoch train loss 0.9779833181509028\n",
      "504-th epoch val loss 0.9661484397607043\n",
      "505-th epoch train loss 0.9779951587304755\n",
      "505-th epoch val loss 0.9661606511945543\n",
      "506-th epoch train loss 0.9780068214535077\n",
      "506-th epoch val loss 0.966172678956726\n",
      "507-th epoch train loss 0.9780183088677554\n",
      "507-th epoch val loss 0.96618452568621\n",
      "508-th epoch train loss 0.9780296234884944\n",
      "508-th epoch val loss 0.9661961939880851\n",
      "509-th epoch train loss 0.9780407677988\n",
      "509-th epoch val loss 0.9662076864338179\n",
      "510-th epoch train loss 0.9780517442498253\n",
      "510-th epoch val loss 0.9662190055615647\n",
      "511-th epoch train loss 0.9780625552610859\n",
      "511-th epoch val loss 0.9662301538764779\n",
      "512-th epoch train loss 0.9780732032207432\n",
      "512-th epoch val loss 0.9662411338510112\n",
      "513-th epoch train loss 0.9780836904858928\n",
      "513-th epoch val loss 0.9662519479252284\n",
      "514-th epoch train loss 0.9780940193828527\n",
      "514-th epoch val loss 0.9662625985071125\n",
      "515-th epoch train loss 0.9781041922074557\n",
      "515-th epoch val loss 0.96627308797288\n",
      "516-th epoch train loss 0.978114211225338\n",
      "516-th epoch val loss 0.9662834186672876\n",
      "517-th epoch train loss 0.9781240786722378\n",
      "517-th epoch val loss 0.9662935929039516\n",
      "518-th epoch train loss 0.9781337967542842\n",
      "518-th epoch val loss 0.9663036129656578\n",
      "519-th epoch train loss 0.9781433676482973\n",
      "519-th epoch val loss 0.9663134811046772\n",
      "520-th epoch train loss 0.9781527935020783\n",
      "520-th epoch val loss 0.9663231995430785\n",
      "521-th epoch train loss 0.978162076434711\n",
      "521-th epoch val loss 0.9663327704730482\n",
      "522-th epoch train loss 0.9781712185368555\n",
      "522-th epoch val loss 0.9663421960571981\n",
      "523-th epoch train loss 0.9781802218710448\n",
      "523-th epoch val loss 0.966351478428887\n",
      "524-th epoch train loss 0.978189088471983\n",
      "524-th epoch val loss 0.9663606196925302\n",
      "525-th epoch train loss 0.9781978203468391\n",
      "525-th epoch val loss 0.966369621923915\n",
      "526-th epoch train loss 0.9782064194755458\n",
      "526-th epoch val loss 0.9663784871705143\n",
      "527-th epoch train loss 0.9782148878110946\n",
      "527-th epoch val loss 0.9663872174518009\n",
      "528-th epoch train loss 0.9782232272798281\n",
      "528-th epoch val loss 0.9663958147595539\n",
      "529-th epoch train loss 0.9782314397817394\n",
      "529-th epoch val loss 0.9664042810581763\n",
      "530-th epoch train loss 0.9782395271907617\n",
      "530-th epoch val loss 0.9664126182849992\n",
      "531-th epoch train loss 0.9782474913550625\n",
      "531-th epoch val loss 0.9664208283505946\n",
      "532-th epoch train loss 0.9782553340973367\n",
      "532-th epoch val loss 0.9664289131390819\n",
      "533-th epoch train loss 0.9782630572150933\n",
      "533-th epoch val loss 0.9664368745084296\n",
      "534-th epoch train loss 0.9782706624809507\n",
      "534-th epoch val loss 0.9664447142907668\n",
      "535-th epoch train loss 0.9782781516429204\n",
      "535-th epoch val loss 0.9664524342926835\n",
      "536-th epoch train loss 0.9782855264246946\n",
      "536-th epoch val loss 0.9664600362955286\n",
      "537-th epoch train loss 0.978292788525933\n",
      "537-th epoch val loss 0.9664675220557158\n",
      "538-th epoch train loss 0.9782999396225488\n",
      "538-th epoch val loss 0.9664748933050203\n",
      "539-th epoch train loss 0.9783069813669845\n",
      "539-th epoch val loss 0.9664821517508699\n",
      "540-th epoch train loss 0.978313915388498\n",
      "540-th epoch val loss 0.9664892990766462\n",
      "541-th epoch train loss 0.9783207432934414\n",
      "541-th epoch val loss 0.9664963369419741\n",
      "542-th epoch train loss 0.9783274666655382\n",
      "542-th epoch val loss 0.9665032669830143\n",
      "543-th epoch train loss 0.9783340870661533\n",
      "543-th epoch val loss 0.9665100908127475\n",
      "544-th epoch train loss 0.978340606034577\n",
      "544-th epoch val loss 0.9665168100212683\n",
      "545-th epoch train loss 0.9783470250882872\n",
      "545-th epoch val loss 0.9665234261760621\n",
      "546-th epoch train loss 0.9783533457232246\n",
      "546-th epoch val loss 0.9665299408222929\n",
      "547-th epoch train loss 0.9783595694140584\n",
      "547-th epoch val loss 0.9665363554830799\n",
      "548-th epoch train loss 0.9783656976144532\n",
      "548-th epoch val loss 0.9665426716597789\n",
      "549-th epoch train loss 0.9783717317573309\n",
      "549-th epoch val loss 0.9665488908322534\n",
      "550-th epoch train loss 0.9783776732551351\n",
      "550-th epoch val loss 0.966555014459153\n",
      "551-th epoch train loss 0.9783835235000884\n",
      "551-th epoch val loss 0.9665610439781809\n",
      "552-th epoch train loss 0.9783892838644505\n",
      "552-th epoch val loss 0.9665669808063638\n",
      "553-th epoch train loss 0.9783949557007731\n",
      "553-th epoch val loss 0.9665728263403185\n",
      "554-th epoch train loss 0.9784005403421528\n",
      "554-th epoch val loss 0.9665785819565149\n",
      "555-th epoch train loss 0.9784060391024829\n",
      "555-th epoch val loss 0.9665842490115398\n",
      "556-th epoch train loss 0.9784114532767004\n",
      "556-th epoch val loss 0.9665898288423532\n",
      "557-th epoch train loss 0.9784167841410331\n",
      "557-th epoch val loss 0.9665953227665467\n",
      "558-th epoch train loss 0.9784220329532427\n",
      "558-th epoch val loss 0.9666007320825974\n",
      "559-th epoch train loss 0.9784272009528688\n",
      "559-th epoch val loss 0.9666060580701216\n",
      "560-th epoch train loss 0.9784322893614641\n",
      "560-th epoch val loss 0.9666113019901192\n",
      "561-th epoch train loss 0.9784372993828355\n",
      "561-th epoch val loss 0.9666164650852264\n",
      "562-th epoch train loss 0.9784422322032766\n",
      "562-th epoch val loss 0.9666215485799569\n",
      "563-th epoch train loss 0.9784470889918011\n",
      "563-th epoch val loss 0.9666265536809442\n",
      "564-th epoch train loss 0.978451870900372\n",
      "564-th epoch val loss 0.9666314815771823\n",
      "565-th epoch train loss 0.9784565790641293\n",
      "565-th epoch val loss 0.9666363334402599\n",
      "566-th epoch train loss 0.9784612146016171\n",
      "566-th epoch val loss 0.9666411104245987\n",
      "567-th epoch train loss 0.9784657786150043\n",
      "567-th epoch val loss 0.9666458136676833\n",
      "568-th epoch train loss 0.978470272190306\n",
      "568-th epoch val loss 0.9666504442902888\n",
      "569-th epoch train loss 0.9784746963976033\n",
      "569-th epoch val loss 0.9666550033967126\n",
      "570-th epoch train loss 0.9784790522912571\n",
      "570-th epoch val loss 0.9666594920749952\n",
      "571-th epoch train loss 0.978483340910124\n",
      "571-th epoch val loss 0.966663911397143\n",
      "572-th epoch train loss 0.978487563277766\n",
      "572-th epoch val loss 0.9666682624193493\n",
      "573-th epoch train loss 0.9784917204026592\n",
      "573-th epoch val loss 0.9666725461822097\n",
      "574-th epoch train loss 0.978495813278402\n",
      "574-th epoch val loss 0.9666767637109377\n",
      "575-th epoch train loss 0.9784998428839192\n",
      "575-th epoch val loss 0.9666809160155773\n",
      "576-th epoch train loss 0.9785038101836625\n",
      "576-th epoch val loss 0.9666850040912128\n",
      "577-th epoch train loss 0.9785077161278125\n",
      "577-th epoch val loss 0.9666890289181744\n",
      "578-th epoch train loss 0.9785115616524742\n",
      "578-th epoch val loss 0.9666929914622462\n",
      "579-th epoch train loss 0.9785153476798738\n",
      "579-th epoch val loss 0.9666968926748666\n",
      "580-th epoch train loss 0.9785190751185517\n",
      "580-th epoch val loss 0.9667007334933312\n",
      "581-th epoch train loss 0.9785227448635486\n",
      "581-th epoch val loss 0.9667045148409845\n",
      "582-th epoch train loss 0.9785263577966026\n",
      "582-th epoch val loss 0.9667082376274237\n",
      "583-th epoch train loss 0.9785299147863273\n",
      "583-th epoch val loss 0.9667119027486858\n",
      "584-th epoch train loss 0.9785334166884008\n",
      "584-th epoch val loss 0.9667155110874412\n",
      "585-th epoch train loss 0.9785368643457435\n",
      "585-th epoch val loss 0.9667190635131809\n",
      "586-th epoch train loss 0.9785402585887018\n",
      "586-th epoch val loss 0.9667225608824034\n",
      "587-th epoch train loss 0.9785436002352219\n",
      "587-th epoch val loss 0.9667260040387974\n",
      "588-th epoch train loss 0.9785468900910277\n",
      "588-th epoch val loss 0.9667293938134269\n",
      "589-th epoch train loss 0.9785501289497925\n",
      "589-th epoch val loss 0.9667327310249062\n",
      "590-th epoch train loss 0.9785533175933091\n",
      "590-th epoch val loss 0.9667360164795789\n",
      "591-th epoch train loss 0.9785564567916613\n",
      "591-th epoch val loss 0.9667392509716943\n",
      "592-th epoch train loss 0.9785595473033871\n",
      "592-th epoch val loss 0.9667424352835778\n",
      "593-th epoch train loss 0.9785625898756464\n",
      "593-th epoch val loss 0.9667455701858023\n",
      "594-th epoch train loss 0.9785655852443824\n",
      "594-th epoch val loss 0.9667486564373579\n",
      "595-th epoch train loss 0.9785685341344811\n",
      "595-th epoch val loss 0.9667516947858156\n",
      "596-th epoch train loss 0.9785714372599303\n",
      "596-th epoch val loss 0.9667546859674936\n",
      "597-th epoch train loss 0.9785742953239777\n",
      "597-th epoch val loss 0.9667576307076179\n",
      "598-th epoch train loss 0.9785771090192826\n",
      "598-th epoch val loss 0.9667605297204828\n",
      "599-th epoch train loss 0.9785798790280706\n",
      "599-th epoch val loss 0.9667633837096092\n",
      "600-th epoch train loss 0.9785826060222821\n",
      "600-th epoch val loss 0.9667661933679\n",
      "601-th epoch train loss 0.9785852906637235\n",
      "601-th epoch val loss 0.9667689593777938\n",
      "602-th epoch train loss 0.9785879336042094\n",
      "602-th epoch val loss 0.966771682411415\n",
      "603-th epoch train loss 0.9785905354857125\n",
      "603-th epoch val loss 0.9667743631307267\n",
      "604-th epoch train loss 0.9785930969405021\n",
      "604-th epoch val loss 0.9667770021876764\n",
      "605-th epoch train loss 0.9785956185912877\n",
      "605-th epoch val loss 0.9667796002243423\n",
      "606-th epoch train loss 0.9785981010513565\n",
      "606-th epoch val loss 0.966782157873077\n",
      "607-th epoch train loss 0.9786005449247104\n",
      "607-th epoch val loss 0.9667846757566491\n",
      "608-th epoch train loss 0.9786029508062036\n",
      "608-th epoch val loss 0.9667871544883845\n",
      "609-th epoch train loss 0.9786053192816734\n",
      "609-th epoch val loss 0.9667895946723031\n",
      "610-th epoch train loss 0.9786076509280737\n",
      "610-th epoch val loss 0.9667919969032562\n",
      "611-th epoch train loss 0.9786099463136033\n",
      "611-th epoch val loss 0.9667943617670605\n",
      "612-th epoch train loss 0.9786122059978373\n",
      "612-th epoch val loss 0.9667966898406305\n",
      "613-th epoch train loss 0.9786144305318483\n",
      "613-th epoch val loss 0.9667989816921086\n",
      "614-th epoch train loss 0.9786166204583382\n",
      "614-th epoch val loss 0.9668012378809971\n",
      "615-th epoch train loss 0.9786187763117554\n",
      "615-th epoch val loss 0.9668034589582829\n",
      "616-th epoch train loss 0.9786208986184184\n",
      "616-th epoch val loss 0.9668056454665624\n",
      "617-th epoch train loss 0.9786229878966365\n",
      "617-th epoch val loss 0.9668077979401685\n",
      "618-th epoch train loss 0.9786250446568258\n",
      "618-th epoch val loss 0.9668099169052898\n",
      "619-th epoch train loss 0.9786270694016274\n",
      "619-th epoch val loss 0.9668120028800917\n",
      "620-th epoch train loss 0.9786290626260232\n",
      "620-th epoch val loss 0.9668140563748375\n",
      "621-th epoch train loss 0.9786310248174453\n",
      "621-th epoch val loss 0.9668160778920017\n",
      "622-th epoch train loss 0.978632956455892\n",
      "622-th epoch val loss 0.9668180679263879\n",
      "623-th epoch train loss 0.9786348580140355\n",
      "623-th epoch val loss 0.9668200269652433\n",
      "624-th epoch train loss 0.9786367299573323\n",
      "624-th epoch val loss 0.9668219554883688\n",
      "625-th epoch train loss 0.9786385727441277\n",
      "625-th epoch val loss 0.9668238539682309\n",
      "626-th epoch train loss 0.978640386825765\n",
      "626-th epoch val loss 0.9668257228700722\n",
      "627-th epoch train loss 0.9786421726466865\n",
      "627-th epoch val loss 0.9668275626520162\n",
      "628-th epoch train loss 0.9786439306445394\n",
      "628-th epoch val loss 0.9668293737651763\n",
      "629-th epoch train loss 0.9786456612502725\n",
      "629-th epoch val loss 0.9668311566537576\n",
      "630-th epoch train loss 0.9786473648882412\n",
      "630-th epoch val loss 0.9668329117551637\n",
      "631-th epoch train loss 0.9786490419763029\n",
      "631-th epoch val loss 0.9668346395000952\n",
      "632-th epoch train loss 0.9786506929259143\n",
      "632-th epoch val loss 0.9668363403126499\n",
      "633-th epoch train loss 0.9786523181422281\n",
      "633-th epoch val loss 0.9668380146104253\n",
      "634-th epoch train loss 0.9786539180241871\n",
      "634-th epoch val loss 0.9668396628046134\n",
      "635-th epoch train loss 0.9786554929646162\n",
      "635-th epoch val loss 0.9668412853000949\n",
      "636-th epoch train loss 0.9786570433503146\n",
      "636-th epoch val loss 0.9668428824955374\n",
      "637-th epoch train loss 0.978658569562147\n",
      "637-th epoch val loss 0.9668444547834869\n",
      "638-th epoch train loss 0.9786600719751324\n",
      "638-th epoch val loss 0.9668460025504608\n",
      "639-th epoch train loss 0.9786615509585297\n",
      "639-th epoch val loss 0.9668475261770376\n",
      "640-th epoch train loss 0.9786630068759284\n",
      "640-th epoch val loss 0.966849026037946\n",
      "641-th epoch train loss 0.97866444008533\n",
      "641-th epoch val loss 0.9668505025021551\n",
      "642-th epoch train loss 0.978665850939234\n",
      "642-th epoch val loss 0.9668519559329584\n",
      "643-th epoch train loss 0.9786672397847205\n",
      "643-th epoch val loss 0.9668533866880608\n",
      "644-th epoch train loss 0.9786686069635321\n",
      "644-th epoch val loss 0.9668547951196639\n",
      "645-th epoch train loss 0.9786699528121543\n",
      "645-th epoch val loss 0.9668561815745479\n",
      "646-th epoch train loss 0.9786712776618929\n",
      "646-th epoch val loss 0.9668575463941518\n",
      "647-th epoch train loss 0.9786725818389563\n",
      "647-th epoch val loss 0.9668588899146582\n",
      "648-th epoch train loss 0.9786738656645282\n",
      "648-th epoch val loss 0.9668602124670694\n",
      "649-th epoch train loss 0.9786751294548467\n",
      "649-th epoch val loss 0.9668615143772874\n",
      "650-th epoch train loss 0.9786763735212775\n",
      "650-th epoch val loss 0.966862795966191\n",
      "651-th epoch train loss 0.9786775981703881\n",
      "651-th epoch val loss 0.9668640575497105\n",
      "652-th epoch train loss 0.9786788037040206\n",
      "652-th epoch val loss 0.9668652994389058\n",
      "653-th epoch train loss 0.9786799904193628\n",
      "653-th epoch val loss 0.9668665219400366\n",
      "654-th epoch train loss 0.9786811586090189\n",
      "654-th epoch val loss 0.9668677253546375\n",
      "655-th epoch train loss 0.978682308561081\n",
      "655-th epoch val loss 0.9668689099795899\n",
      "656-th epoch train loss 0.9786834405591927\n",
      "656-th epoch val loss 0.9668700761071907\n",
      "657-th epoch train loss 0.9786845548826226\n",
      "657-th epoch val loss 0.9668712240252252\n",
      "658-th epoch train loss 0.9786856518063269\n",
      "658-th epoch val loss 0.966872354017032\n",
      "659-th epoch train loss 0.9786867316010155\n",
      "659-th epoch val loss 0.966873466361574\n",
      "660-th epoch train loss 0.9786877945332179\n",
      "660-th epoch val loss 0.9668745613335026\n",
      "661-th epoch train loss 0.9786888408653454\n",
      "661-th epoch val loss 0.9668756392032246\n",
      "662-th epoch train loss 0.9786898708557554\n",
      "662-th epoch val loss 0.966876700236967\n",
      "663-th epoch train loss 0.9786908847588106\n",
      "663-th epoch val loss 0.9668777446968388\n",
      "664-th epoch train loss 0.9786918828249425\n",
      "664-th epoch val loss 0.9668787728408967\n",
      "665-th epoch train loss 0.9786928653007092\n",
      "665-th epoch val loss 0.9668797849232055\n",
      "666-th epoch train loss 0.9786938324288568\n",
      "666-th epoch val loss 0.966880781193898\n",
      "667-th epoch train loss 0.9786947844483755\n",
      "667-th epoch val loss 0.9668817618992371\n",
      "668-th epoch train loss 0.9786957215945586\n",
      "668-th epoch val loss 0.9668827272816733\n",
      "669-th epoch train loss 0.9786966440990571\n",
      "669-th epoch val loss 0.9668836775799047\n",
      "670-th epoch train loss 0.9786975521899359\n",
      "670-th epoch val loss 0.9668846130289314\n",
      "671-th epoch train loss 0.9786984460917303\n",
      "671-th epoch val loss 0.9668855338601151\n",
      "672-th epoch train loss 0.9786993260254979\n",
      "672-th epoch val loss 0.9668864403012339\n",
      "673-th epoch train loss 0.9787001922088729\n",
      "673-th epoch val loss 0.9668873325765356\n",
      "674-th epoch train loss 0.9787010448561184\n",
      "674-th epoch val loss 0.9668882109067948\n",
      "675-th epoch train loss 0.9787018841781772\n",
      "675-th epoch val loss 0.966889075509363\n",
      "676-th epoch train loss 0.9787027103827236\n",
      "676-th epoch val loss 0.9668899265982227\n",
      "677-th epoch train loss 0.9787035236742133\n",
      "677-th epoch val loss 0.9668907643840386\n",
      "678-th epoch train loss 0.9787043242539314\n",
      "678-th epoch val loss 0.9668915890742085\n",
      "679-th epoch train loss 0.9787051123200441\n",
      "679-th epoch val loss 0.9668924008729145\n",
      "680-th epoch train loss 0.9787058880676426\n",
      "680-th epoch val loss 0.9668931999811697\n",
      "681-th epoch train loss 0.9787066516887943\n",
      "681-th epoch val loss 0.9668939865968719\n",
      "682-th epoch train loss 0.9787074033725857\n",
      "682-th epoch val loss 0.9668947609148435\n",
      "683-th epoch train loss 0.9787081433051689\n",
      "683-th epoch val loss 0.9668955231268859\n",
      "684-th epoch train loss 0.9787088716698098\n",
      "684-th epoch val loss 0.9668962734218238\n",
      "685-th epoch train loss 0.9787095886469284\n",
      "685-th epoch val loss 0.9668970119855494\n",
      "686-th epoch train loss 0.9787102944141445\n",
      "686-th epoch val loss 0.9668977390010696\n",
      "687-th epoch train loss 0.9787109891463202\n",
      "687-th epoch val loss 0.9668984546485476\n",
      "688-th epoch train loss 0.9787116730156032\n",
      "688-th epoch val loss 0.9668991591053498\n",
      "689-th epoch train loss 0.9787123461914674\n",
      "689-th epoch val loss 0.9668998525460892\n",
      "690-th epoch train loss 0.9787130088407544\n",
      "690-th epoch val loss 0.9669005351426623\n",
      "691-th epoch train loss 0.9787136611277142\n",
      "691-th epoch val loss 0.9669012070642968\n",
      "692-th epoch train loss 0.9787143032140456\n",
      "692-th epoch val loss 0.9669018684775911\n",
      "693-th epoch train loss 0.9787149352589343\n",
      "693-th epoch val loss 0.9669025195465532\n",
      "694-th epoch train loss 0.9787155574190926\n",
      "694-th epoch val loss 0.9669031604326422\n",
      "695-th epoch train loss 0.9787161698487967\n",
      "695-th epoch val loss 0.9669037912948077\n",
      "696-th epoch train loss 0.9787167726999253\n",
      "696-th epoch val loss 0.9669044122895268\n",
      "697-th epoch train loss 0.978717366121995\n",
      "697-th epoch val loss 0.9669050235708441\n",
      "698-th epoch train loss 0.9787179502621991\n",
      "698-th epoch val loss 0.9669056252904087\n",
      "699-th epoch train loss 0.9787185252654406\n",
      "699-th epoch val loss 0.9669062175975102\n",
      "700-th epoch train loss 0.9787190912743698\n",
      "700-th epoch val loss 0.9669068006391168\n",
      "701-th epoch train loss 0.9787196484294189\n",
      "701-th epoch val loss 0.9669073745599103\n",
      "702-th epoch train loss 0.9787201968688338\n",
      "702-th epoch val loss 0.9669079395023206\n",
      "703-th epoch train loss 0.9787207367287106\n",
      "703-th epoch val loss 0.9669084956065613\n",
      "704-th epoch train loss 0.9787212681430282\n",
      "704-th epoch val loss 0.9669090430106645\n",
      "705-th epoch train loss 0.9787217912436792\n",
      "705-th epoch val loss 0.966909581850513\n",
      "706-th epoch train loss 0.9787223061605057\n",
      "706-th epoch val loss 0.9669101122598758\n",
      "707-th epoch train loss 0.9787228130213275\n",
      "707-th epoch val loss 0.9669106343704384\n",
      "708-th epoch train loss 0.9787233119519744\n",
      "708-th epoch val loss 0.9669111483118359\n",
      "709-th epoch train loss 0.9787238030763186\n",
      "709-th epoch val loss 0.9669116542116861\n",
      "710-th epoch train loss 0.9787242865163021\n",
      "710-th epoch val loss 0.9669121521956173\n",
      "711-th epoch train loss 0.9787247623919688\n",
      "711-th epoch val loss 0.9669126423873037\n",
      "712-th epoch train loss 0.9787252308214939\n",
      "712-th epoch val loss 0.9669131249084906\n",
      "713-th epoch train loss 0.9787256919212113\n",
      "713-th epoch val loss 0.9669135998790294\n",
      "714-th epoch train loss 0.9787261458056421\n",
      "714-th epoch val loss 0.9669140674169018\n",
      "715-th epoch train loss 0.9787265925875266\n",
      "715-th epoch val loss 0.9669145276382551\n",
      "716-th epoch train loss 0.9787270323778446\n",
      "716-th epoch val loss 0.9669149806574227\n",
      "717-th epoch train loss 0.9787274652858496\n",
      "717-th epoch val loss 0.9669154265869583\n",
      "718-th epoch train loss 0.978727891419091\n",
      "718-th epoch val loss 0.9669158655376608\n",
      "719-th epoch train loss 0.9787283108834445\n",
      "719-th epoch val loss 0.9669162976186039\n",
      "720-th epoch train loss 0.9787287237831332\n",
      "720-th epoch val loss 0.966916722937159\n",
      "721-th epoch train loss 0.9787291302207576\n",
      "721-th epoch val loss 0.9669171415990255\n",
      "722-th epoch train loss 0.9787295302973182\n",
      "722-th epoch val loss 0.9669175537082533\n",
      "723-th epoch train loss 0.9787299241122415\n",
      "723-th epoch val loss 0.9669179593672719\n",
      "724-th epoch train loss 0.9787303117634035\n",
      "724-th epoch val loss 0.9669183586769122\n",
      "725-th epoch train loss 0.9787306933471543\n",
      "725-th epoch val loss 0.9669187517364334\n",
      "726-th epoch train loss 0.9787310689583413\n",
      "726-th epoch val loss 0.9669191386435464\n",
      "727-th epoch train loss 0.9787314386903336\n",
      "727-th epoch val loss 0.9669195194944389\n",
      "728-th epoch train loss 0.9787318026350434\n",
      "728-th epoch val loss 0.9669198943837976\n",
      "729-th epoch train loss 0.9787321608829506\n",
      "729-th epoch val loss 0.9669202634048327\n",
      "730-th epoch train loss 0.9787325135231222\n",
      "730-th epoch val loss 0.9669206266492996\n",
      "731-th epoch train loss 0.9787328606432388\n",
      "731-th epoch val loss 0.9669209842075245\n",
      "732-th epoch train loss 0.9787332023296094\n",
      "732-th epoch val loss 0.9669213361684212\n",
      "733-th epoch train loss 0.9787335386671986\n",
      "733-th epoch val loss 0.966921682619518\n",
      "734-th epoch train loss 0.978733869739646\n",
      "734-th epoch val loss 0.9669220236469779\n",
      "735-th epoch train loss 0.9787341956292845\n",
      "735-th epoch val loss 0.9669223593356174\n",
      "736-th epoch train loss 0.9787345164171639\n",
      "736-th epoch val loss 0.9669226897689317\n",
      "737-th epoch train loss 0.9787348321830682\n",
      "737-th epoch val loss 0.9669230150291112\n",
      "738-th epoch train loss 0.9787351430055362\n",
      "738-th epoch val loss 0.9669233351970637\n",
      "739-th epoch train loss 0.978735448961881\n",
      "739-th epoch val loss 0.9669236503524342\n",
      "740-th epoch train loss 0.9787357501282103\n",
      "740-th epoch val loss 0.9669239605736258\n",
      "741-th epoch train loss 0.9787360465794421\n",
      "741-th epoch val loss 0.9669242659378156\n",
      "742-th epoch train loss 0.9787363383893256\n",
      "742-th epoch val loss 0.9669245665209768\n",
      "743-th epoch train loss 0.9787366256304585\n",
      "743-th epoch val loss 0.9669248623978958\n",
      "744-th epoch train loss 0.9787369083743057\n",
      "744-th epoch val loss 0.9669251536421927\n",
      "745-th epoch train loss 0.9787371866912155\n",
      "745-th epoch val loss 0.966925440326337\n",
      "746-th epoch train loss 0.9787374606504381\n",
      "746-th epoch val loss 0.966925722521666\n",
      "747-th epoch train loss 0.9787377303201432\n",
      "747-th epoch val loss 0.9669260002984047\n",
      "748-th epoch train loss 0.9787379957674339\n",
      "748-th epoch val loss 0.9669262737256786\n",
      "749-th epoch train loss 0.9787382570583686\n",
      "749-th epoch val loss 0.9669265428715373\n",
      "750-th epoch train loss 0.97873851425797\n",
      "750-th epoch val loss 0.9669268078029644\n",
      "751-th epoch train loss 0.9787387674302489\n",
      "751-th epoch val loss 0.9669270685858989\n",
      "752-th epoch train loss 0.9787390166382143\n",
      "752-th epoch val loss 0.9669273252852489\n",
      "753-th epoch train loss 0.9787392619438916\n",
      "753-th epoch val loss 0.966927577964909\n",
      "754-th epoch train loss 0.9787395034083372\n",
      "754-th epoch val loss 0.9669278266877752\n",
      "755-th epoch train loss 0.978739741091654\n",
      "755-th epoch val loss 0.966928071515762\n",
      "756-th epoch train loss 0.9787399750530044\n",
      "756-th epoch val loss 0.9669283125098148\n",
      "757-th epoch train loss 0.9787402053506293\n",
      "757-th epoch val loss 0.9669285497299283\n",
      "758-th epoch train loss 0.9787404320418577\n",
      "758-th epoch val loss 0.9669287832351594\n",
      "759-th epoch train loss 0.978740655183123\n",
      "759-th epoch val loss 0.966929013083641\n",
      "760-th epoch train loss 0.978740874829978\n",
      "760-th epoch val loss 0.9669292393325998\n",
      "761-th epoch train loss 0.9787410910371052\n",
      "761-th epoch val loss 0.966929462038366\n",
      "762-th epoch train loss 0.9787413038583351\n",
      "762-th epoch val loss 0.9669296812563907\n",
      "763-th epoch train loss 0.9787415133466544\n",
      "763-th epoch val loss 0.9669298970412578\n",
      "764-th epoch train loss 0.9787417195542242\n",
      "764-th epoch val loss 0.9669301094466983\n",
      "765-th epoch train loss 0.9787419225323893\n",
      "765-th epoch val loss 0.9669303185256041\n",
      "766-th epoch train loss 0.9787421223316913\n",
      "766-th epoch val loss 0.9669305243300391\n",
      "767-th epoch train loss 0.9787423190018827\n",
      "767-th epoch val loss 0.9669307269112535\n",
      "768-th epoch train loss 0.9787425125919385\n",
      "768-th epoch val loss 0.9669309263196975\n",
      "769-th epoch train loss 0.9787427031500675\n",
      "769-th epoch val loss 0.9669311226050312\n",
      "770-th epoch train loss 0.9787428907237251\n",
      "770-th epoch val loss 0.9669313158161403\n",
      "771-th epoch train loss 0.9787430753596252\n",
      "771-th epoch val loss 0.9669315060011443\n",
      "772-th epoch train loss 0.9787432571037512\n",
      "772-th epoch val loss 0.9669316932074111\n",
      "773-th epoch train loss 0.9787434360013673\n",
      "773-th epoch val loss 0.9669318774815678\n",
      "774-th epoch train loss 0.9787436120970302\n",
      "774-th epoch val loss 0.9669320588695131\n",
      "775-th epoch train loss 0.9787437854346\n",
      "775-th epoch val loss 0.966932237416427\n",
      "776-th epoch train loss 0.9787439560572512\n",
      "776-th epoch val loss 0.9669324131667841\n",
      "777-th epoch train loss 0.9787441240074829\n",
      "777-th epoch val loss 0.9669325861643627\n",
      "778-th epoch train loss 0.9787442893271296\n",
      "778-th epoch val loss 0.9669327564522573\n",
      "779-th epoch train loss 0.978744452057372\n",
      "779-th epoch val loss 0.9669329240728869\n",
      "780-th epoch train loss 0.9787446122387466\n",
      "780-th epoch val loss 0.9669330890680093\n",
      "781-th epoch train loss 0.9787447699111564\n",
      "781-th epoch val loss 0.9669332514787282\n",
      "782-th epoch train loss 0.9787449251138792\n",
      "782-th epoch val loss 0.9669334113455036\n",
      "783-th epoch train loss 0.9787450778855799\n",
      "783-th epoch val loss 0.966933568708164\n",
      "784-th epoch train loss 0.9787452282643172\n",
      "784-th epoch val loss 0.9669337236059143\n",
      "785-th epoch train loss 0.9787453762875569\n",
      "785-th epoch val loss 0.9669338760773469\n",
      "786-th epoch train loss 0.9787455219921759\n",
      "786-th epoch val loss 0.9669340261604491\n",
      "787-th epoch train loss 0.9787456654144759\n",
      "787-th epoch val loss 0.9669341738926154\n",
      "788-th epoch train loss 0.9787458065901916\n",
      "788-th epoch val loss 0.9669343193106548\n",
      "789-th epoch train loss 0.9787459455544965\n",
      "789-th epoch val loss 0.9669344624508\n",
      "790-th epoch train loss 0.9787460823420159\n",
      "790-th epoch val loss 0.9669346033487177\n",
      "791-th epoch train loss 0.9787462169868323\n",
      "791-th epoch val loss 0.9669347420395168\n",
      "792-th epoch train loss 0.9787463495224957\n",
      "792-th epoch val loss 0.9669348785577564\n",
      "793-th epoch train loss 0.9787464799820301\n",
      "793-th epoch val loss 0.9669350129374534\n",
      "794-th epoch train loss 0.9787466083979446\n",
      "794-th epoch val loss 0.9669351452120966\n",
      "795-th epoch train loss 0.9787467348022383\n",
      "795-th epoch val loss 0.9669352754146467\n",
      "796-th epoch train loss 0.9787468592264095\n",
      "796-th epoch val loss 0.9669354035775499\n",
      "797-th epoch train loss 0.9787469817014656\n",
      "797-th epoch val loss 0.966935529732747\n",
      "798-th epoch train loss 0.9787471022579274\n",
      "798-th epoch val loss 0.966935653911676\n",
      "799-th epoch train loss 0.9787472209258375\n",
      "799-th epoch val loss 0.9669357761452843\n",
      "800-th epoch train loss 0.9787473377347699\n",
      "800-th epoch val loss 0.9669358964640347\n",
      "801-th epoch train loss 0.9787474527138341\n",
      "801-th epoch val loss 0.966936014897913\n",
      "802-th epoch train loss 0.9787475658916872\n",
      "802-th epoch val loss 0.9669361314764385\n",
      "803-th epoch train loss 0.9787476772965342\n",
      "803-th epoch val loss 0.9669362462286643\n",
      "804-th epoch train loss 0.9787477869561411\n",
      "804-th epoch val loss 0.9669363591831915\n",
      "805-th epoch train loss 0.9787478948978382\n",
      "805-th epoch val loss 0.9669364703681729\n",
      "806-th epoch train loss 0.9787480011485293\n",
      "806-th epoch val loss 0.9669365798113208\n",
      "807-th epoch train loss 0.9787481057346964\n",
      "807-th epoch val loss 0.9669366875399145\n",
      "808-th epoch train loss 0.978748208682406\n",
      "808-th epoch val loss 0.9669367935808044\n",
      "809-th epoch train loss 0.9787483100173189\n",
      "809-th epoch val loss 0.9669368979604227\n",
      "810-th epoch train loss 0.9787484097646928\n",
      "810-th epoch val loss 0.9669370007047875\n",
      "811-th epoch train loss 0.978748507949391\n",
      "811-th epoch val loss 0.966937101839509\n",
      "812-th epoch train loss 0.9787486045958862\n",
      "812-th epoch val loss 0.9669372013897962\n",
      "813-th epoch train loss 0.9787486997282697\n",
      "813-th epoch val loss 0.966937299380465\n",
      "814-th epoch train loss 0.9787487933702553\n",
      "814-th epoch val loss 0.9669373958359428\n",
      "815-th epoch train loss 0.9787488855451846\n",
      "815-th epoch val loss 0.9669374907802726\n",
      "816-th epoch train loss 0.9787489762760347\n",
      "816-th epoch val loss 0.9669375842371227\n",
      "817-th epoch train loss 0.9787490655854234\n",
      "817-th epoch val loss 0.9669376762297911\n",
      "818-th epoch train loss 0.9787491534956136\n",
      "818-th epoch val loss 0.9669377667812098\n",
      "819-th epoch train loss 0.9787492400285198\n",
      "819-th epoch val loss 0.966937855913953\n",
      "820-th epoch train loss 0.9787493252057147\n",
      "820-th epoch val loss 0.9669379436502414\n",
      "821-th epoch train loss 0.978749409048432\n",
      "821-th epoch val loss 0.9669380300119474\n",
      "822-th epoch train loss 0.9787494915775728\n",
      "822-th epoch val loss 0.9669381150206013\n",
      "823-th epoch train loss 0.978749572813713\n",
      "823-th epoch val loss 0.9669381986973964\n",
      "824-th epoch train loss 0.9787496527771045\n",
      "824-th epoch val loss 0.9669382810631946\n",
      "825-th epoch train loss 0.9787497314876814\n",
      "825-th epoch val loss 0.9669383621385288\n",
      "826-th epoch train loss 0.9787498089650688\n",
      "826-th epoch val loss 0.9669384419436142\n",
      "827-th epoch train loss 0.9787498852285815\n",
      "827-th epoch val loss 0.9669385204983469\n",
      "828-th epoch train loss 0.9787499602972327\n",
      "828-th epoch val loss 0.9669385978223113\n",
      "829-th epoch train loss 0.9787500341897403\n",
      "829-th epoch val loss 0.9669386739347878\n",
      "830-th epoch train loss 0.9787501069245252\n",
      "830-th epoch val loss 0.9669387488547517\n",
      "831-th epoch train loss 0.9787501785197231\n",
      "831-th epoch val loss 0.9669388226008837\n",
      "832-th epoch train loss 0.9787502489931847\n",
      "832-th epoch val loss 0.9669388951915711\n",
      "833-th epoch train loss 0.9787503183624804\n",
      "833-th epoch val loss 0.9669389666449125\n",
      "834-th epoch train loss 0.9787503866449064\n",
      "834-th epoch val loss 0.9669390369787245\n",
      "835-th epoch train loss 0.978750453857488\n",
      "835-th epoch val loss 0.9669391062105436\n",
      "836-th epoch train loss 0.9787505200169841\n",
      "836-th epoch val loss 0.9669391743576333\n",
      "837-th epoch train loss 0.9787505851398911\n",
      "837-th epoch val loss 0.9669392414369852\n",
      "838-th epoch train loss 0.9787506492424456\n",
      "838-th epoch val loss 0.9669393074653241\n",
      "839-th epoch train loss 0.9787507123406326\n",
      "839-th epoch val loss 0.9669393724591159\n",
      "840-th epoch train loss 0.9787507744501849\n",
      "840-th epoch val loss 0.9669394364345658\n",
      "841-th epoch train loss 0.978750835586589\n",
      "841-th epoch val loss 0.9669394994076268\n",
      "842-th epoch train loss 0.9787508957650903\n",
      "842-th epoch val loss 0.9669395613940018\n",
      "843-th epoch train loss 0.9787509550006945\n",
      "843-th epoch val loss 0.9669396224091478\n",
      "844-th epoch train loss 0.9787510133081719\n",
      "844-th epoch val loss 0.966939682468279\n",
      "845-th epoch train loss 0.9787510707020629\n",
      "845-th epoch val loss 0.9669397415863723\n",
      "846-th epoch train loss 0.9787511271966775\n",
      "846-th epoch val loss 0.9669397997781689\n",
      "847-th epoch train loss 0.9787511828061053\n",
      "847-th epoch val loss 0.9669398570581819\n",
      "848-th epoch train loss 0.9787512375442129\n",
      "848-th epoch val loss 0.966939913440694\n",
      "849-th epoch train loss 0.9787512914246499\n",
      "849-th epoch val loss 0.9669399689397649\n",
      "850-th epoch train loss 0.9787513444608527\n",
      "850-th epoch val loss 0.9669400235692353\n",
      "851-th epoch train loss 0.9787513966660464\n",
      "851-th epoch val loss 0.9669400773427284\n",
      "852-th epoch train loss 0.9787514480532503\n",
      "852-th epoch val loss 0.9669401302736537\n",
      "853-th epoch train loss 0.9787514986352792\n",
      "853-th epoch val loss 0.9669401823752121\n",
      "854-th epoch train loss 0.9787515484247468\n",
      "854-th epoch val loss 0.9669402336603956\n",
      "855-th epoch train loss 0.9787515974340705\n",
      "855-th epoch val loss 0.9669402841419955\n",
      "856-th epoch train loss 0.9787516456754709\n",
      "856-th epoch val loss 0.9669403338325995\n",
      "857-th epoch train loss 0.9787516931609798\n",
      "857-th epoch val loss 0.9669403827446006\n",
      "858-th epoch train loss 0.9787517399024388\n",
      "858-th epoch val loss 0.966940430890197\n",
      "859-th epoch train loss 0.9787517859115052\n",
      "859-th epoch val loss 0.9669404782813958\n",
      "860-th epoch train loss 0.9787518311996526\n",
      "860-th epoch val loss 0.9669405249300153\n",
      "861-th epoch train loss 0.9787518757781755\n",
      "861-th epoch val loss 0.96694057084769\n",
      "862-th epoch train loss 0.9787519196581914\n",
      "862-th epoch val loss 0.9669406160458718\n",
      "863-th epoch train loss 0.9787519628506449\n",
      "863-th epoch val loss 0.9669406605358329\n",
      "864-th epoch train loss 0.9787520053663067\n",
      "864-th epoch val loss 0.9669407043286691\n",
      "865-th epoch train loss 0.9787520472157804\n",
      "865-th epoch val loss 0.9669407474353017\n",
      "866-th epoch train loss 0.9787520884095038\n",
      "866-th epoch val loss 0.9669407898664826\n",
      "867-th epoch train loss 0.9787521289577492\n",
      "867-th epoch val loss 0.9669408316327928\n",
      "868-th epoch train loss 0.9787521688706319\n",
      "868-th epoch val loss 0.9669408727446516\n",
      "869-th epoch train loss 0.9787522081581045\n",
      "869-th epoch val loss 0.9669409132123101\n",
      "870-th epoch train loss 0.9787522468299655\n",
      "870-th epoch val loss 0.9669409530458624\n",
      "871-th epoch train loss 0.9787522848958611\n",
      "871-th epoch val loss 0.966940992255244\n",
      "872-th epoch train loss 0.9787523223652841\n",
      "872-th epoch val loss 0.9669410308502331\n",
      "873-th epoch train loss 0.9787523592475806\n",
      "873-th epoch val loss 0.9669410688404559\n",
      "874-th epoch train loss 0.9787523955519492\n",
      "874-th epoch val loss 0.9669411062353886\n",
      "875-th epoch train loss 0.9787524312874446\n",
      "875-th epoch val loss 0.966941143044357\n",
      "876-th epoch train loss 0.9787524664629795\n",
      "876-th epoch val loss 0.966941179276542\n",
      "877-th epoch train loss 0.9787525010873277\n",
      "877-th epoch val loss 0.9669412149409813\n",
      "878-th epoch train loss 0.9787525351691254\n",
      "878-th epoch val loss 0.9669412500465708\n",
      "879-th epoch train loss 0.9787525687168726\n",
      "879-th epoch val loss 0.9669412846020647\n",
      "880-th epoch train loss 0.9787526017389372\n",
      "880-th epoch val loss 0.9669413186160837\n",
      "881-th epoch train loss 0.9787526342435549\n",
      "881-th epoch val loss 0.9669413520971106\n",
      "882-th epoch train loss 0.9787526662388341\n",
      "882-th epoch val loss 0.9669413850534968\n",
      "883-th epoch train loss 0.9787526977327543\n",
      "883-th epoch val loss 0.9669414174934626\n",
      "884-th epoch train loss 0.978752728733172\n",
      "884-th epoch val loss 0.9669414494250996\n",
      "885-th epoch train loss 0.978752759247818\n",
      "885-th epoch val loss 0.9669414808563718\n",
      "886-th epoch train loss 0.9787527892843045\n",
      "886-th epoch val loss 0.9669415117951194\n",
      "887-th epoch train loss 0.9787528188501232\n",
      "887-th epoch val loss 0.9669415422490597\n",
      "888-th epoch train loss 0.9787528479526486\n",
      "888-th epoch val loss 0.9669415722257885\n",
      "889-th epoch train loss 0.9787528765991402\n",
      "889-th epoch val loss 0.9669416017327834\n",
      "890-th epoch train loss 0.9787529047967429\n",
      "890-th epoch val loss 0.966941630777404\n",
      "891-th epoch train loss 0.9787529325524905\n",
      "891-th epoch val loss 0.9669416593668954\n",
      "892-th epoch train loss 0.9787529598733057\n",
      "892-th epoch val loss 0.9669416875083889\n",
      "893-th epoch train loss 0.9787529867660036\n",
      "893-th epoch val loss 0.9669417152089042\n",
      "894-th epoch train loss 0.9787530132372921\n",
      "894-th epoch val loss 0.9669417424753497\n",
      "895-th epoch train loss 0.9787530392937746\n",
      "895-th epoch val loss 0.9669417693145286\n",
      "896-th epoch train loss 0.9787530649419502\n",
      "896-th epoch val loss 0.966941795733134\n",
      "897-th epoch train loss 0.9787530901882165\n",
      "897-th epoch val loss 0.9669418217377571\n",
      "898-th epoch train loss 0.978753115038871\n",
      "898-th epoch val loss 0.9669418473348834\n",
      "899-th epoch train loss 0.9787531395001123\n",
      "899-th epoch val loss 0.9669418725308984\n",
      "900-th epoch train loss 0.9787531635780431\n",
      "900-th epoch val loss 0.9669418973320879\n",
      "901-th epoch train loss 0.9787531872786681\n",
      "901-th epoch val loss 0.966941921744637\n",
      "902-th epoch train loss 0.9787532106078995\n",
      "902-th epoch val loss 0.9669419457746362\n",
      "903-th epoch train loss 0.9787532335715576\n",
      "903-th epoch val loss 0.9669419694280794\n",
      "904-th epoch train loss 0.9787532561753696\n",
      "904-th epoch val loss 0.9669419927108666\n",
      "905-th epoch train loss 0.9787532784249746\n",
      "905-th epoch val loss 0.966942015628807\n",
      "906-th epoch train loss 0.9787533003259224\n",
      "906-th epoch val loss 0.966942038187616\n",
      "907-th epoch train loss 0.9787533218836765\n",
      "907-th epoch val loss 0.9669420603929216\n",
      "908-th epoch train loss 0.9787533431036136\n",
      "908-th epoch val loss 0.9669420822502623\n",
      "909-th epoch train loss 0.9787533639910281\n",
      "909-th epoch val loss 0.9669421037650913\n",
      "910-th epoch train loss 0.9787533845511296\n",
      "910-th epoch val loss 0.9669421249427749\n",
      "911-th epoch train loss 0.9787534047890473\n",
      "911-th epoch val loss 0.9669421457885965\n",
      "912-th epoch train loss 0.9787534247098296\n",
      "912-th epoch val loss 0.9669421663077556\n",
      "913-th epoch train loss 0.9787534443184456\n",
      "913-th epoch val loss 0.9669421865053716\n",
      "914-th epoch train loss 0.9787534636197871\n",
      "914-th epoch val loss 0.9669422063864819\n",
      "915-th epoch train loss 0.9787534826186681\n",
      "915-th epoch val loss 0.9669422259560463\n",
      "916-th epoch train loss 0.9787535013198286\n",
      "916-th epoch val loss 0.9669422452189469\n",
      "917-th epoch train loss 0.9787535197279342\n",
      "917-th epoch val loss 0.9669422641799889\n",
      "918-th epoch train loss 0.9787535378475758\n",
      "918-th epoch val loss 0.9669422828439023\n",
      "919-th epoch train loss 0.9787535556832745\n",
      "919-th epoch val loss 0.9669423012153427\n",
      "920-th epoch train loss 0.978753573239479\n",
      "920-th epoch val loss 0.9669423192988934\n",
      "921-th epoch train loss 0.9787535905205692\n",
      "921-th epoch val loss 0.9669423370990655\n",
      "922-th epoch train loss 0.9787536075308555\n",
      "922-th epoch val loss 0.9669423546202989\n",
      "923-th epoch train loss 0.978753624274582\n",
      "923-th epoch val loss 0.966942371866965\n",
      "924-th epoch train loss 0.9787536407559253\n",
      "924-th epoch val loss 0.9669423888433665\n",
      "925-th epoch train loss 0.9787536569789962\n",
      "925-th epoch val loss 0.9669424055537372\n",
      "926-th epoch train loss 0.978753672947843\n",
      "926-th epoch val loss 0.9669424220022473\n",
      "927-th epoch train loss 0.9787536886664484\n",
      "927-th epoch val loss 0.9669424381929989\n",
      "928-th epoch train loss 0.9787537041387339\n",
      "928-th epoch val loss 0.9669424541300311\n",
      "929-th epoch train loss 0.9787537193685598\n",
      "929-th epoch val loss 0.9669424698173208\n",
      "930-th epoch train loss 0.9787537343597246\n",
      "930-th epoch val loss 0.9669424852587802\n",
      "931-th epoch train loss 0.9787537491159686\n",
      "931-th epoch val loss 0.9669425004582624\n",
      "932-th epoch train loss 0.9787537636409729\n",
      "932-th epoch val loss 0.9669425154195582\n",
      "933-th epoch train loss 0.9787537779383606\n",
      "933-th epoch val loss 0.9669425301464003\n",
      "934-th epoch train loss 0.978753792011699\n",
      "934-th epoch val loss 0.9669425446424634\n",
      "935-th epoch train loss 0.9787538058644986\n",
      "935-th epoch val loss 0.9669425589113623\n",
      "936-th epoch train loss 0.9787538195002153\n",
      "936-th epoch val loss 0.9669425729566578\n",
      "937-th epoch train loss 0.9787538329222506\n",
      "937-th epoch val loss 0.9669425867818531\n",
      "938-th epoch train loss 0.9787538461339533\n",
      "938-th epoch val loss 0.9669426003903978\n",
      "939-th epoch train loss 0.9787538591386189\n",
      "939-th epoch val loss 0.9669426137856866\n",
      "940-th epoch train loss 0.9787538719394919\n",
      "940-th epoch val loss 0.9669426269710607\n",
      "941-th epoch train loss 0.9787538845397661\n",
      "941-th epoch val loss 0.9669426399498106\n",
      "942-th epoch train loss 0.9787538969425849\n",
      "942-th epoch val loss 0.9669426527251734\n",
      "943-th epoch train loss 0.9787539091510415\n",
      "943-th epoch val loss 0.9669426653003366\n",
      "944-th epoch train loss 0.9787539211681819\n",
      "944-th epoch val loss 0.9669426776784361\n",
      "945-th epoch train loss 0.9787539329970046\n",
      "945-th epoch val loss 0.9669426898625617\n",
      "946-th epoch train loss 0.9787539446404607\n",
      "946-th epoch val loss 0.9669427018557522\n",
      "947-th epoch train loss 0.9787539561014545\n",
      "947-th epoch val loss 0.9669427136609996\n",
      "948-th epoch train loss 0.9787539673828455\n",
      "948-th epoch val loss 0.9669427252812494\n",
      "949-th epoch train loss 0.9787539784874476\n",
      "949-th epoch val loss 0.9669427367193997\n",
      "950-th epoch train loss 0.9787539894180314\n",
      "950-th epoch val loss 0.9669427479783046\n",
      "951-th epoch train loss 0.978754000177324\n",
      "951-th epoch val loss 0.9669427590607731\n",
      "952-th epoch train loss 0.9787540107680092\n",
      "952-th epoch val loss 0.9669427699695694\n",
      "953-th epoch train loss 0.9787540211927289\n",
      "953-th epoch val loss 0.9669427807074157\n",
      "954-th epoch train loss 0.9787540314540844\n",
      "954-th epoch val loss 0.9669427912769898\n",
      "955-th epoch train loss 0.9787540415546362\n",
      "955-th epoch val loss 0.9669428016809305\n",
      "956-th epoch train loss 0.9787540514969031\n",
      "956-th epoch val loss 0.9669428119218315\n",
      "957-th epoch train loss 0.978754061283366\n",
      "957-th epoch val loss 0.9669428220022486\n",
      "958-th epoch train loss 0.9787540709164665\n",
      "958-th epoch val loss 0.9669428319246965\n",
      "959-th epoch train loss 0.978754080398607\n",
      "959-th epoch val loss 0.96694284169165\n",
      "960-th epoch train loss 0.9787540897321546\n",
      "960-th epoch val loss 0.9669428513055475\n",
      "961-th epoch train loss 0.9787540989194365\n",
      "961-th epoch val loss 0.9669428607687853\n",
      "962-th epoch train loss 0.9787541079627449\n",
      "962-th epoch val loss 0.9669428700837253\n",
      "963-th epoch train loss 0.9787541168643364\n",
      "963-th epoch val loss 0.9669428792526918\n",
      "964-th epoch train loss 0.9787541256264324\n",
      "964-th epoch val loss 0.9669428882779719\n",
      "965-th epoch train loss 0.9787541342512175\n",
      "965-th epoch val loss 0.9669428971618174\n",
      "966-th epoch train loss 0.978754142740844\n",
      "966-th epoch val loss 0.9669429059064438\n",
      "967-th epoch train loss 0.9787541510974302\n",
      "967-th epoch val loss 0.966942914514034\n",
      "968-th epoch train loss 0.9787541593230604\n",
      "968-th epoch val loss 0.9669429229867348\n",
      "969-th epoch train loss 0.9787541674197872\n",
      "969-th epoch val loss 0.9669429313266601\n",
      "970-th epoch train loss 0.9787541753896298\n",
      "970-th epoch val loss 0.9669429395358901\n",
      "971-th epoch train loss 0.978754183234578\n",
      "971-th epoch val loss 0.9669429476164739\n",
      "972-th epoch train loss 0.9787541909565871\n",
      "972-th epoch val loss 0.966942955570426\n",
      "973-th epoch train loss 0.9787541985575853\n",
      "973-th epoch val loss 0.9669429633997326\n",
      "974-th epoch train loss 0.9787542060394677\n",
      "974-th epoch val loss 0.966942971106345\n",
      "975-th epoch train loss 0.9787542134041024\n",
      "975-th epoch val loss 0.9669429786921879\n",
      "976-th epoch train loss 0.978754220653325\n",
      "976-th epoch val loss 0.9669429861591525\n",
      "977-th epoch train loss 0.9787542277889453\n",
      "977-th epoch val loss 0.966942993509102\n",
      "978-th epoch train loss 0.978754234812743\n",
      "978-th epoch val loss 0.9669430007438701\n",
      "979-th epoch train loss 0.97875424172647\n",
      "979-th epoch val loss 0.9669430078652614\n",
      "980-th epoch train loss 0.9787542485318521\n",
      "980-th epoch val loss 0.9669430148750536\n",
      "981-th epoch train loss 0.9787542552305866\n",
      "981-th epoch val loss 0.9669430217749948\n",
      "982-th epoch train loss 0.9787542618243453\n",
      "982-th epoch val loss 0.9669430285668065\n",
      "983-th epoch train loss 0.9787542683147726\n",
      "983-th epoch val loss 0.966943035252184\n",
      "984-th epoch train loss 0.9787542747034887\n",
      "984-th epoch val loss 0.9669430418327943\n",
      "985-th epoch train loss 0.978754280992086\n",
      "985-th epoch val loss 0.9669430483102789\n",
      "986-th epoch train loss 0.9787542871821343\n",
      "986-th epoch val loss 0.9669430546862547\n",
      "987-th epoch train loss 0.978754293275178\n",
      "987-th epoch val loss 0.9669430609623116\n",
      "988-th epoch train loss 0.9787542992727375\n",
      "988-th epoch val loss 0.9669430671400162\n",
      "989-th epoch train loss 0.9787543051763087\n",
      "989-th epoch val loss 0.9669430732209096\n",
      "990-th epoch train loss 0.9787543109873641\n",
      "990-th epoch val loss 0.9669430792065082\n",
      "991-th epoch train loss 0.9787543167073542\n",
      "991-th epoch val loss 0.9669430850983064\n",
      "992-th epoch train loss 0.9787543223377058\n",
      "992-th epoch val loss 0.9669430908977734\n",
      "993-th epoch train loss 0.9787543278798234\n",
      "993-th epoch val loss 0.966943096606356\n",
      "994-th epoch train loss 0.9787543333350894\n",
      "994-th epoch val loss 0.9669431022254789\n",
      "995-th epoch train loss 0.9787543387048661\n",
      "995-th epoch val loss 0.966943107756544\n",
      "996-th epoch train loss 0.9787543439904917\n",
      "996-th epoch val loss 0.9669431132009308\n",
      "997-th epoch train loss 0.9787543491932863\n",
      "997-th epoch val loss 0.9669431185599983\n",
      "998-th epoch train loss 0.978754354314547\n",
      "998-th epoch val loss 0.9669431238350832\n",
      "999-th epoch train loss 0.978754359355552\n",
      "999-th epoch val loss 0.9669431290275013\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc77c75a-6e53-4c20-9a5a-0c7d1d6f3d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ddbead8130>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0c2ed13-97fb-4231-a0d2-0acc375faada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.mean((y-y_pred)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9452c20-02e4-4ea8-bab2-70a64f1cc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3227044d-1a14-43c6-adca-a2b3c1e13a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56782113877.70742"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
